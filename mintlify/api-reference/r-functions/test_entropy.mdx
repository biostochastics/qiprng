---
title: 'test_entropy'
description: 'Shannon entropy test'
sidebarTitle: 'test_entropy'
---

# Shannon entropy test


 Tests the information content of a random sequence using Shannon entropy.
 High entropy indicates good randomness, while low entropy suggests patterns
 or predictability in the data.


## Usage

```r

 test_entropy(samples, bins = 256)

```

## Parameters

<ParamField body="samples" type="any">
  Numeric vector of random samples to test
</ParamField>
<ParamField body="bins" type="any">
  Number of bins for discretization (default: 256)
</ParamField>

## Returns


 A list containing entropy test results:

 test_name Name of the test
 shannon_entropy Calculated Shannon entropy in bits
 max_entropy Maximum possible entropy for given bin count
 normalized_entropy Ratio of observed to maximum entropy (0-1)
 passed Logical; TRUE if normalized entropy > 0.95
 interpretation Human-readable interpretation of results



## Details


 The Shannon entropy is calculated as:
 H = -\sum_{i=1}^{n} p_i \log_2(p_i)

 where p_i is the probability of bin i. The samples are discretized into
 the specified number of bins, and the entropy is calculated from the
 resulting frequency distribution.

 For a perfectly uniform distribution, the entropy equals log2(bins).
 The test passes if the normalized entropy (H/H_max) exceeds 0.95,
 indicating high randomness.


## Examples

```r


 # Test entropy of uniform random numbers
 samples <- runif(10000)
 result <- test_entropy(samples)
 print(paste("Normalized entropy:", round(result$normalized_entropy, 4)))

 # Test with fewer bins
 result <- test_entropy(samples, bins = 128)


```
