# Technical Documentation for the Quadratic Irrational PRNG v0.6.6

 **Attribution Note**: The mathematical foundation and core algorithm design presented in this document is based on the amazing work of Vincent Granville (2024), specifically his research on military-grade random number generators using quadratic irrationals that I ran into in the dead of night after having way too much coffee. While Granville provided the original Python implementation, this doc describes my R/C++ implementation developed primarily for educational purposes. Cool if you find it useful. Even better if you find a bug!

 **Reference**: Granville, V. (2022). Synthetic Data and Generative AI 1st Edition. <https://mltechniques.com/2022/12/13/military-grade-fast-random-number-generator-based-on-quadratic-irrationals/>

## Version 0.5.0 Enhancements

### Matrix Jump-Ahead with O(log n) Complexity

The v0.5.0 implementation includes an optimized jump-ahead algorithm using matrix exponentiation:

```cpp
// Matrix representation of the quadratic map
[ x_{n+1} ]   [ a  b  c ] [ x_n^2 ]
[ x_n     ] = [ 1  0  0 ] [ x_n   ]
[ 1       ]   [ 0  0  1 ] [ 1     ]
```

Using binary exponentiation, we can compute M^n in O(log n) operations, enabling efficient parallel stream generation.

### Advanced Mixing Strategies

Five mixing strategies combine outputs from multiple QI generators:

1. **Round Robin**: Sequential cycling through generators
2. **XOR Mixing**: Bitwise XOR of mantissas for maximum entropy
3. **Averaging**: Weighted average for smooth distribution
4. **Modular Addition**: Sum modulo 1 for entropy combining
5. **Cascade Mixing**: Three-pass mixing (XOR â†’ modular â†’ nonlinear)

### SIMD Vectorization

Hardware-optimized operations using:

- **AVX2/AVX512**: 4-8 doubles per operation on x86
- **ARM NEON**: 2 doubles per operation on Apple Silicon
- **Automatic fallback**: Scalar operations when SIMD unavailable

## 1. Mathematical Foundation: The Quadratic Map

### 1.1 Basic Principle

The core algorithm, as described by Granville (2024), is based on a quadratic recurrence relation of the form:

```
x_{n+1} = (a*x_n^2 + b*x_n + c) mod 1
```

Where:

- `x_n` is the current state (a value in [0,1])
- `a`, `b`, and `c` are integer parameters
- `mod 1` means we take the fractional part of the result

This is a discrete dynamical system derived from the theory of quadratic irrationals. When properly configured, this map exhibits chaotic behavior, making it suitable for random number generation.

### 1.2 The Discriminant Requirement

The discriminant of the quadratic equation is defined as:

```
Î” = bÂ² - 4ac
```

**Why the discriminant must be positive:**

1. **Mathematical reasons**: A positive discriminant ensures that the quadratic equation `axÂ² + bx + c = 0` has two distinct real roots. This is essential because:

   - When Î” > 0, the quadratic map has two fixed points in the real domain
   - These fixed points create a bounded region where chaotic behavior occurs
   - The sequence will "bounce" unpredictably between these boundaries

2. **Randomness quality**: When the discriminant is positive, the sequence exhibits sensitive dependence on initial conditions (the "butterfly effect"), a hallmark of chaotic systems that produces high-quality randomness.

3. **Sequence behavior**:
   - Negative discriminant: The sequence would converge to a predictable pattern
   - Zero discriminant: The sequence would gravitate toward a single fixed point
   - Only a positive discriminant creates the necessary "stretching and folding" dynamic for chaos

### 1.3 Orbit Structure

The sequence generated by this recurrence relation creates what mathematicians call an "orbit" in dynamical systems theory. With proper parameters, this orbit:

- Densely fills the interval [0,1]
- Never repeats exactly (with sufficient precision)
- Has a uniform distribution of points
- Exhibits exponential divergence of nearby starting values

## 2. Parameter Constraints and Their Reasoning

### 2.1 Parameter `a`

**Constraint**: `a > 0` (must be positive)

**Reasoning**:

- Controls the "stretching" factor of the map
- If negative, would invert the parabola and destroy the chaotic properties
- If zero, would degenerate into a linear map with poor randomness

### 2.2 Parameter `c`

**Constraint**: `c < 0` (must be negative)

**Reasoning**:

- Together with `a > 0`, ensures the parabola crosses the x-axis in the right places
- Creates the correct topology for the chaotic attractor
- Ensures the mapping stays within [0,1] after taking the fractional part

### 2.3 Discriminant: `bÂ² - 4ac > 0`

**Reasoning**:

- As explained above, ensures two real fixed points
- Creates the mathematical foundation for chaotic behavior
- Prevents the sequence from falling into simple patterns

### 2.4 Parameter Selection Guidelines

For optimal randomness properties:

1. **Coprime values**: Ideally, parameters `a`, `b`, and `c` should be coprime (no common divisors)
2. **Moderate values**: While any valid values will work mathematically, moderate values (e.g., 1-10) are more numerically stable
3. **High precision**: Higher MPFR precision increases the period length and quality of randomness

## 3. MultiQI Implementation (v0.5.0 Enhanced)

### 3.1 Mixing Strategies

Version 0.5.0 introduces configurable mixing strategies for combining multiple QI sequences:

1. **Round-Robin** (Default):
   - Cycles through sequences sequentially
   - Lowest computational overhead
   - Good for general-purpose applications

2. **XOR Mixing**:
   - Combines outputs via bitwise XOR of mantissas
   - Excellent bit diffusion properties
   - Ideal for cryptographic applications

3. **Averaging**:
   - Averages outputs from all QI sequences
   - Smooths distribution characteristics
   - Reduces individual sequence biases

4. **Modular Addition**:
   - Adds outputs modulo 1
   - Preserves uniform distribution
   - Good entropy mixing

5. **Cascade Mixing**:
   - Feeds output of one QI as input to next
   - Maximum entropy diffusion
   - Highest security but slower performance

### 3.2 Mathematical Benefits

Each mixing strategy provides different theoretical advantages:

- **Period extension**: Effective period up to LCM of individual periods
- **Entropy amplification**: Multiple sources increase unpredictability
- **Bias reduction**: Averaging reduces systematic biases
- **Correlation breaking**: XOR destroys linear correlations

### 3.2 Parameter Diversification

The implementation uses a carefully selected set of (a,b,c) parameters:

1. **Parameter selection**:
   - Parameters are chosen to have positive discriminants
   - Different parameters create sequences with different orbital characteristics
   - Parameter selection affects the "mixing speed" of the sequence

2. **Random skipping**:
   - Each QI sequence is initialized and then skipped a random number of steps
   - This ensures different starting points even with the same parameters
   - Prevents potential synchronization of sequences

3. **Manual Multi-QI Configuration** (v0.5.0):
   - Users can specify multiple QI parameters directly via vectors
   - Example: `a = c(2, 3, 5), b = c(7, 11, 13), c = c(-3, -5, -7)`
   - Each triplet (a[i], b[i], c[i]) defines a separate QI generator
   - All discriminants are validated to ensure mathematical correctness
   - Supports 2-16 simultaneous QI generators for optimal performance

### 3.3 Custom Discriminants via CSV

The implementation supports using custom discriminants from a CSV file:

1. **Mathematical advantages of custom discriminants**:
   - **Optimized orbital characteristics**: Carefully chosen discriminants can produce sequences with improved statistical properties
   - **Controlled cycle lengths**: Different discriminants lead to different cycle lengths, allowing optimization for specific applications
   - **Spectral properties**: The discriminant value directly influences the spectral properties of the sequence, with certain values producing better results in frequency domain tests
   - **Greater unpredictability**: Custom discriminants can enhance the unpredictability of the sequence, making it harder to reverse-engineer

2. **Selection criteria for optimal discriminants**:
   - **Prime or prime-rich factorizations**
   - **Congruence classes**
   - **Large discriminants**
   - **Spectral testing**: Discriminants that perform well on spectral tests minimize periodic patterns

3. **Implementation advantages**:
   - **Customization**: Users can tune the PRNG to specific application requirements
   - **Reproducibility**: Custom discriminants allow for reproducible sequences across different environments
   - **Parallel independence**: Using different discriminants for parallel streams ensures statistical independence
   - **Domain-specific optimization**: Applications with special requirements can benefit from discriminants optimized for those needs

4. **Usage example**:

   ```
   # Content of discriminants.csv
   a,b,c,Discriminant
   1,9,-143,653
   1,9,-145,661
   1,9,-148,673
   1,9,-149,677
   1,11,-145,701
   ...
   ```

   The CSV file provides complete (a,b,c) triplets along with their calculated discriminant values (bÂ² - 4ac). This format allows for direct specification of parameters known to have desirable properties, rather than deriving them from discriminants alone.

By default the package uses only the 370 "Excellent" discriminants obtained using testing the 750 discriminants provided with the package.

## 4. Matrix Jump-Ahead Algorithm (v0.5.0)

### 4.1 Mathematical Foundation

The quadratic map can be represented in matrix form. Given the recurrence:

```
x_{n+1} = (aÂ·x_nÂ² + bÂ·x_n + c) mod 1
```

**Key Insight**: While a single quadratic map is nonlinear, we can encode the transformation using a lifted state vector that tracks the polynomial coefficients. The core algebraic fact is:

> The set of quadratic polynomials under composition forms a semigroup that can be represented via matrix multiplication on their coefficient vectors in a suitable basis.

For a quadratic map f(x) = axÂ² + bx + c, its n-fold composition fâˆ˜â¿(x) is a polynomial of degree 2â¿. By lifting to a higher-dimensional state that tracks monomials (or using projective coordinates), we can express:

```
v_{n+1} = M Â· v_n
```

where vâ‚™ encodes the current polynomial's coefficients (or a lifted state) and M is a fixed matrix.

### 4.2 State Vector Representation

The implementation uses a state vector that tracks the relevant polynomial terms:

```
[ x_{n+1}  ]   [ a  b  c ] [ x_nÂ²  ]
[ x_n      ] = [ 1  0  0 ] [ x_n   ]
[ 1        ]   [ 0  0  1 ] [ 1     ]
```

Once we have a representation v_{n+1} = M Â· v_n, then:

```
v_n = M^n Â· v_0
```

### 4.3 Binary Exponentiation

To jump ahead by n steps, we compute Mâ¿ using binary exponentiation (repeated squaring):

```
M^n = M^(b_k Â· 2^k + ... + b_1 Â· 2 + b_0)
    = M^(b_k Â· 2^k) Â· ... Â· M^(b_1 Â· 2) Â· M^b_0
```

**Complexity Analysis**:
- **Naive iteration**: O(n) applications of the map
- **Binary exponentiation**: O(log n) matrix multiplications

This enables efficient jump-ahead for astronomically large n (e.g., 10Â¹â¸ steps).

### 4.4 Mathematical Validity

The "matrix exponentiation â‡’ jump ahead in O(log n)" claim is mathematically sound provided:

1. The chosen matrix representation correctly encodes composition of the map
2. Modular reduction (mod 1) is applied only at the final evaluation stage
3. All linear algebra is performed over reals (or MPFR with sufficient precision)

### 4.5 MPFR Precision Preservation

All matrix operations use MPFR arithmetic to maintain full precision:

- Matrix multiplication preserves exact values
- Modular reduction maintains fractional precision
- No accuracy loss even for astronomical jumps (10Â¹â¸ steps)
- The linear algebra itself operates over MPFR reals, not modular arithmetic

## 5. CFE Period Detection (v0.5.0)

### 5.1 Continued Fraction Expansion

For quadratic irrationals of the form:

```
Î± = (âˆšD + P) / Q
```

The continued fraction expansion is:

```
Î± = a_0 + 1/(a_1 + 1/(a_2 + 1/(...)))
```

### 5.2 Theoretical Foundation: Lagrange's Theorem

**Definition**: A *quadratic irrational* is a real number Î± that is a root of AxÂ² + Bx + C = 0 with integers A, B, C, A â‰  0, and discriminant D = BÂ² - 4AC not a perfect square.

**Theorem (Lagrange)**: A real number has an eventually periodic simple continued fraction expansion if and only if it is a quadratic irrational.

Therefore, numbers of the form Î± = (âˆšD + Pâ‚€)/Qâ‚€ (with integer D > 0 not a perfect square) are exactly in this class, and their CFE is ultimately periodic.

### 5.3 Gauss-Legendre Algorithm

The package implements the Gauss-Legendre algorithm for O(L) period detection:

1. **Initialization**: Start with Pâ‚€ = 0, Qâ‚€ = 1
2. **Iteration**:

   ```
   aâ‚™ = floor((âˆšD + Pâ‚™) / Qâ‚™)
   P_{n+1} = aâ‚™ Â· Qâ‚™ - Pâ‚™
   Q_{n+1} = (D - P_{n+1}Â²) / Qâ‚™
   ```

3. **Period detection**: When (Pâ‚™, Qâ‚™) repeats, period is found

### 5.4 Why the Algorithm Terminates (Proof of Periodicity)

**Theorem (Periodicity of CFE)**: For Î± = (âˆšD + Pâ‚€)/Qâ‚€ with integer D > 0 not a square, the recursion on (Pâ‚™, Qâ‚™) visits only finitely many integer states. Hence (Pâ‚™, Qâ‚™) must eventually repeat, and the resulting CFE [aâ‚€; aâ‚, aâ‚‚, ...] is eventually periodic.

**Proof**: The crucial observation is that (Pâ‚™, Qâ‚™) live in a finite set of integer pairs:

1. **Boundedness of Qâ‚™**: It can be shown that Qâ‚™ divides D - Pâ‚™Â², and the algorithm maintains Q_{n+1} = (D - P_{n+1}Â²)/Qâ‚™ as an integer. The values |Qâ‚™| are bounded by D.

2. **Boundedness of Pâ‚™**: By the definition of the algorithm, |Pâ‚™| â‰¤ âˆšD for all n â‰¥ 1.

3. **Pigeonhole argument**: Since there are only finitely many integer pairs (P, Q) with |P| â‰¤ âˆšD and |Q| â‰¤ D, by the pigeonhole principle some pair must repeat: (Pâ‚™, Qâ‚™) = (Pâ‚˜, Qâ‚˜) with n > m.

4. **Periodicity follows**: Once the state (Pâ‚™, Qâ‚™) repeats, the sequence of partial quotients aâ‚™ repeats from that point onward, establishing the period. âˆ

### 5.5 Complexity Analysis

- **Time complexity**: O(L) where L is the period length
- **Space complexity**: O(L) to store the period
- **Practical performance**: Periods typically < 1000 for discriminants < 10â¶
- **Theoretical bound**: L = O(âˆšD log D) for discriminant D

## 6. Dynamic QI Generation (v0.5.0)

### 6.1 Discriminant Pattern

The package uses discriminants of the form D = kÂ² + 1:

- **Often square-free**: Many values of kÂ² + 1 are square-free, but this is *not* guaranteed
- **Counterexample**: For k = 7, we have kÂ² + 1 = 50 = 2 Ã— 5Â², which is *not* square-free
- Produces well-distributed quadratic irrationals when square-free
- Efficient validation: check if D - 1 is a perfect square

**Important**: The implementation validates square-freeness and discards discriminants that fail this test. The kÂ² + 1 pattern is a convenient source of candidate discriminants, not a guarantee of square-freeness.

### 6.2 Square-Free Validation

Algorithm for checking square-free property:

```
is_square_free(D):
  for p in small_primes:
    if D % (p*p) == 0:
      return false
  return true
```

This validation is **required** for all discriminants, including those of the form kÂ² + 1.

### 6.3 Benefits

- **Quality assurance**: Square-free validation ensures only valid discriminants are used
- **Efficient generation**: O(âˆšD) validation time
- **Good coverage**: kÂ² + 1 pattern provides a rich source of candidate discriminants

## 7. Cryptographic Mixing

### 7.1 Purpose and Motivation

While the quadratic irrational generator produces sequences with good statistical properties, adding cryptographic mixing provides:

1. **Enhanced statistical quality**: Eliminates subtle patterns that might exist in the raw sequence
2. **Cryptographic security**: Makes the output suitable for cryptographic applications
3. **Defense against analysis**: Prevents potential mathematical analysis of the underlying sequence

### 7.2 Mixing Approaches

The package implements two different approaches for cryptographic mixing:

#### 7.2.1 Partial Modular Addition (Default)

```
x_mixed = (x_original + crypto_uniform) mod 1
```

Where:

- `x_original` is the raw value from the quadratic irrational generator
- `crypto_uniform` is a cryptographically secure random value in [0,1)
- The result is taken modulo 1 to keep it in [0,1)

**Theorem (Crypto Mixing Preserves Uniformity)**: If U ~ Unif[0,1) and C ~ Unif[0,1) are independent, then (U + C) mod 1 ~ Unif[0,1).

**Proof**: Let V := (U + C) mod 1. We want to show V ~ Unif[0,1).

Take any measurable set A âŠ‚ [0,1). Then:

```
P(V âˆˆ A) = P((U + C) mod 1 âˆˆ A)
         = âˆ«âˆ« ğŸ™{(u+c) mod 1 âˆˆ A} f_U(u) f_C(c) du dc
```

Since U, C are independent uniforms: f_U(u) = f_C(c) = 1 on [0,1).

Use the fact that adding a fixed c modulo 1 is just a rotation of the circle:

For each fixed c, the map u â†¦ (u + c) mod 1 is a bijection of [0,1) onto itself that preserves Lebesgue measure. Therefore, for uniform u:

```
P((U + c) mod 1 âˆˆ A) = |A|
```

where |A| is the Lebesgue measure of A.

Thus:
```
P(V âˆˆ A) = âˆ«â‚€Â¹ P((U + c) mod 1 âˆˆ A) dc = âˆ«â‚€Â¹ |A| dc = |A|
```

But P(V âˆˆ A) = |A| is exactly the defining property of Unif[0,1). âˆ

**Corollary**: Even if X_original is only approximately uniform, the mixing operation pushes the distribution closer to uniform, since the cryptographic component C is exactly uniform.

**Benefits**:

- **Mathematically proven** to preserve the uniform distribution
- Cryptographically strong
- Efficiently eliminates patterns in the original sequence

#### 7.2.2 Partial Averaging (Alternative)

An alternative approach that uses weighted averaging instead of modular addition, activated with the `adhoc_corrections` flag:

```
x_mixed = (x_original + crypto_uniform) / 2
```

**Important Mathematical Note**: Unlike modular addition, averaging does **not** preserve uniform distribution. If U, C ~ Unif[0,1) are independent, then (U + C)/2 has a triangular distribution on [0,1), not a uniform distribution. The density is:

```
f(x) = 4x       for 0 â‰¤ x â‰¤ 1/2
f(x) = 4(1-x)   for 1/2 < x â‰¤ 1
```

**Differences**:

- Produces a non-uniform (triangular) distribution concentrated toward 0.5
- May be useful for specific applications requiring central tendency
- Included as an alternative mixing strategy for testing purposes

### 7.3 Integration with Core Generator

The cryptographic mixing integrates with the core generator in several ways:

1. **Buffer-based processing**:
   - Values are generated in buffers for efficiency
   - Entire buffers are cryptographically processed at once

2. **Reseeding mechanism**:
   - Both the quadratic parameters and the encryption key are replaced during reseeding
   - Reseeding can occur automatically after a specified number of generated values

3. **Careful handling of sensitive data**:
   - Encryption keys are stored in secure memory
   - Keys are properly zeroed when no longer needed using sodium_memzero
   - Side-channel mitigation techniques are employed

### 7.4 Tie-Breaking Mechanism

The implementation includes a tie-breaking mechanism to handle the rare case of duplicate consecutive values:

1. **Detection**:
   - Checks if the current output exactly equals the previous output
   - Such exact duplications are extremely rare but possible

2. **Resolution**:
   - Adds a tiny random epsilon value to break the tie
   - The epsilon is small enough not to affect the distribution
   - Ensures consecutive identical values never occur

3. **Benefits**:
   - Prevents potential issues in applications that expect unique values
   - Provides additional entropy in edge cases
   - Maintains the overall statistical properties

## 8. Distribution Transformations

The raw uniform [0,1] values can be transformed into various probability distributions:

### 8.1 Uniform Range

Transforms uniform [0,1] to uniform [min, max]:

```
y = min + x*(max - min)
```

### 8.2 Normal Distribution

Uses the Box-Muller transform to generate normally distributed values:

```
r = sqrt(-2*ln(u1))
Î¸ = 2Ï€*u2
x = mean + sd * r*cos(Î¸)
y = mean + sd * r*sin(Î¸)
```

Where u1 and u2 are independent uniform [0,1] values. The implementation optimizes this by storing the second value (y) for the next call, effectively using both random numbers generated from each pair of uniform draws.

### 8.3 Exponential Distribution

Uses the inverse transform method:

```
x = -ln(1-u)/Î»
```

Where u is a uniform [0,1] value and Î» is the rate parameter.

## 9. Precision and Numerical Considerations

### 9.1 MPFR Precision

The package uses the GNU MPFR library for arbitrary-precision arithmetic:

- **Default**: 53 bits (equivalent to double precision)
- **Range**: 24 to 10,000 bits (package limit)
- **Impact**: Higher precision increases cycle length and improves statistical properties

### 9.2 Numerical Stability

Considerations for numerical stability:

- **Overflow protection**: All calculations use arbitrary precision to prevent overflow
- **Underflow handling**: Special handling for values close to zero
- **Modular reduction**: Exact fractional part calculation ensures values stay in [0,1)

### 9.3 Period Length

With sufficient precision, the period length of the generator is extremely long:

- At 53-bit precision: effectively non-repeating for practical applications
- At higher precisions: period grows exponentially
- With cryptographic mixing: period becomes cryptographically large

### 9.4 Fixed Point Avoidance

A critical consideration in the implementation is avoiding fixed points. A fixed point is a value x where f(x) = x, meaning the sequence would get stuck at this value.

**Issues with Fixed Points:**

- Early implementations using the fractional part of 1/(ax + b) could converge to fixed points (e.g., 0.1861407...)
- Once convergence occurs, random numbers are no longer generated, as all subsequent values are identical
- This severely impacts the statistical properties and utility of the PRNG

**Fixed Point Analysis:**

For the map f(x) = {axÂ² + bx + c} (where {Â·} denotes fractional part), a fixed point in [0,1) is any x and integer k such that:

```
axÂ² + bx + c = x + k  âŸº  axÂ² + (b-1)x + (c-k) = 0
```

with x âˆˆ [0,1). For given integers a > 0, b, c < 0, nothing prevents some integer k from making this quadratic have a root in [0,1). **The constraints a > 0, c < 0, Î” > 0 alone do not mathematically guarantee the absence of fixed points.**

**Practical Mitigation:**

1. The current implementation uses the full quadratic map (axÂ² + bx + c mod 1)
2. For our selected parameter sets, empirical testing shows no convergence to fixed points
3. Cryptographic mixing provides strong protection: even if a fixed point were approached, the mixing would immediately move the state away
4. Random initial values and periodic reseeding further prevent any potential fixed point convergence
5. Fully ruling out fixed points for all parameter sets would require a separate, more technical argument specific to each parameter choice

**Note**: The combination of empirical validation, cryptographic mixing, and reseeding makes fixed point convergence practically impossible, even though we do not have a blanket mathematical proof for all parameter combinations.

## 10. Thread Safety and Parallelism

### 10.1 Thread Safety Mechanisms

The package provides two approaches to thread safety:

1. **Global Mutex Protection**:
   - When `use_threading = FALSE` (default)
   - A global mutex protects access to the PRNG state
   - Ensures only one thread can access the PRNG at a time
   - Guarantees sequence reproducibility but may limit parallelism

2. **Thread-Local Instances**:
   - When `use_threading = TRUE`
   - Each thread gets its own independent PRNG instance
   - No mutex contention, allowing full parallelism
   - Different random sequences in each thread

### 10.2 Mathematical Implications of Threading

Using thread-local PRNG instances has important mathematical implications:

1. **Independence**:
   - Sequences generated in different threads are statistically independent
   - No correlation between sequences across threads
   - Reduces potential for pattern emergence in parallel applications

2. **Reproducibility**:
   - With global mutex: sequence is reproducible regardless of threading
   - With thread-local instances: each thread's sequence is reproducible but different
   - Important consideration for scientific computing and simulation

3. **Performance vs. Determinism**:
   - Thread-local instances offer better performance but less predictability
   - Global mutex offers deterministic results but may be slower in highly threaded applications
   - Choice depends on application requirements

## 11. Secure Memory Management

### 11.1 SecureBuffer Implementation

The package uses a template class called `SecureBuffer` for secure memory management:

1. **RAII Design**:
   - Resource Acquisition Is Initialization pattern
   - Automatically handles proper cleanup of sensitive data
   - Prevents memory leaks and security vulnerabilities

2. **Secure Memory Clearing**:
   - Overwrites memory with zeros before deallocation
   - Uses volatile pointers to prevent compiler optimization
   - Ensures sensitive data (like encryption keys) is securely erased

3. **Memory Safety**:
   - Type-safe wrapper around std::vector
   - Prevents buffer overflows and memory corruption
   - Simplifies memory management throughout the codebase

### 11.2 Side-Channel Mitigation

The implementation includes several measures to reduce side-channel attack vectors:

1. **Constant-time operations** where possible
2. **Avoidance of secret-dependent control flow**
3. **Protection against timing attacks**
4. **Use of libsodium's secure memory functions**

### 11.3 Application to Sensitive Data

SecureBuffer is applied to:

- Cryptographic keys
- Nonces
- Temporary buffers containing sensitive data
- Any other security-critical components

## 12. Implementation Note on Use of SecureBuffer

The qiprng implementation uses the SecureBuffer template class for secure memory management of sensitive data. This class:

1. Provides RAII-style memory management, automatically clearing sensitive data when it goes out of scope
2. Implements a secure clearing procedure that overwrites memory with zeros before deallocation
3. Ensures that cryptographic keys and other sensitive values are properly handled

When implementing or modifying code, it's important to:

- Always use SecureBuffer for any security-sensitive data
- Avoid creating unnecessary copies of sensitive data
- Allow the SecureBuffer destructor to handle clearing instead of manual clearing
- Remember that SecureBuffer constructor takes only a size parameter, not an initializer value

## 13. Memory Management Optimizations (v0.5.0)

### 13.1 MPFR Memory Pooling

Version 0.5.0 introduces a singleton memory pool for MPFR objects:

1. **Pool Architecture**:
   - Pre-allocated pool of 1024 MPFR objects
   - Lock-free allocation using atomic operations
   - Zero-copy returns to pool

2. **Performance Benefits**:
   - Eliminates malloc/free overhead
   - Reduces memory fragmentation
   - Improves cache locality

3. **Thread Safety**:
   - Thread-safe allocation/deallocation
   - Per-thread quotas prevent starvation
   - Fallback to heap allocation when pool exhausted

### 13.2 Thread-Local Storage

Each thread maintains its own MultiQI instance:

1. **ThreadManager Singleton**:
   - Manages thread-local QI instances
   - Automatic cleanup on thread exit
   - Prevents race conditions

2. **Benefits**:
   - No mutex contention for generation
   - Better cache performance
   - Scalable to many threads

3. **Memory Efficiency**:
   - Lazy initialization per thread
   - Automatic cleanup prevents leaks
   - Shared discriminant data

## 14. Discriminants (pre-selected)

The package includes a set of pre-selected discriminants for quadratic irrationals.

### Excellent Discriminant Selection (Production Default)

**New Default Behavior**: qiprng now uses only the 370 "Excellent" rated discriminants by default, identified through comprehensive analysis.

**Selection Criteria** (all must be satisfied):

- **Autocorrelation Excellence**: `max_abs_acf â‰¤ 0.010`
- **Zero Significant Lags**: No autocorrelation beyond statistical/empirical thresholds
- **Multi-Test Validation**: Pass uniformity, independence, and periodicity tests

**Parameter Patterns for Excellence**:

- **Parameter `a`**: Prefer small values (1-3) for optimal performance
- **Discriminant range**: 1000-2000 shows consistently high quality
- **Parameter `c`**: Moderately negative values (-120 to -150) preferred

**Quality Assurance**: Each excellent discriminant achieved:

- Overall quality score â‰¥ 0.85 ("Excellent" threshold)
- Passed comprehensive statistical validation including autocorrelation, uniformity, independence, and periodicity tests
- Demonstrated stable performance across 1,000,000 sample sequences

The package includes all discriminants for research, but uses only excellent ones for production unless explicitly configured otherwise. Legacy discriminants are selected based on:

### Comprehensive Discriminant Analysis Results

**Executive Summary**: A comprehensive analysis of all 750 discriminants, each tested with 1,000,000 samples, was performed. The results confirm that while the underlying generation method is strong, rigorous testing is crucial for identifying the highest quality parameter sets.

- **370 Excellent Discriminants**: A total of 370 discriminants (49.3%) passed all tests with the highest quality rating.
- **Complete Quality Distribution**: Excellent: 370 (49.3%), Very-Good: 31 (4.1%), Good: 293 (39.1%), Fair: 50 (6.7%), Poor: 6 (0.8%)
- **Effective Discrimination**: The stricter autocorrelation test provides a more realistic distribution of results, successfully identifying minor flaws that were previously missed.
- **Overall Robustness**: 92.5% of the discriminants were rated as "Good" or better (including "Very-Good"), confirming the underlying strength of the generation method.

## 15. Performance Analysis

### 15.1 Benchmarking Results

In comprehensive benchmarks comparing qiprng with standard R generators, the following performance characteristics were observed:

| Generator | Time for 100K samples | Relative Speed | Statistical Quality |
|-----------|----------------------|----------------|-------------------|
| qiprng | ~27ms | 1x (baseline) | Excellent (cryptographic) |
| Mersenne Twister | ~0.5ms | 54x faster | Excellent (non-crypto) |
| L'Ecuyer-CMRG | ~0.9ms | 30x faster | Very Good |
| Wichmann-Hill | ~0.4ms | 68x faster | Good |

### 15.2 Computational Complexity Analysis

The performance difference stems from fundamental algorithmic differences:

#### 15.2.1 Operation Count per Random Number

**Standard Linear Congruential Generators (e.g., Mersenne Twister)**:

- 1 integer multiplication
- 1 integer addition
- 1 modulo operation (often optimized away)
- Total: ~3 simple integer operations

**qiprng Quadratic Irrational Generator**:

- 1 MPFR squaring operation (mpfr_mul)
- 2 MPFR scalar multiplications (mpfr_mul_si)
- 2 MPFR additions (mpfr_add, mpfr_add_si)
- 1 MPFR fractional part extraction (mpfr_frac)
- 1 MPFR sign check and conditional addition
- Total: ~7 arbitrary precision operations

#### 15.2.2 MPFR Overhead

Each MPFR operation involves:

- **Dynamic memory management**: Arbitrary precision requires dynamic allocation
- **Software arithmetic**: No hardware acceleration for arbitrary precision
- **Rounding control**: Each operation must respect the rounding mode
- **Exception handling**: NaN and infinity checks after each operation

Typical overhead: 100-1000x slower than hardware floating-point operations

### 15.3 Component-wise Performance Breakdown

Based on code analysis and profiling estimates:

1. **MPFR Arithmetic Operations** (~60% of runtime)
   - Dominated by mpfr_mul for squaring
   - Fractional part extraction (mpfr_frac) is particularly expensive

2. **Cryptographic Mixing** (~20% when enabled)
   - libsodium ChaCha20 generation
   - Modular addition or XOR operations
   - Periodic reseeding

3. **Thread Safety** (~10%)
   - Mutex acquisition/release
   - Thread-local storage access
   - Memory barriers for synchronization

4. **Buffer Management** (~5%)
   - Buffer refill operations
   - Memory copying
   - Position tracking

5. **Error Checking & Validation** (~5%)
   - Parameter validation
   - NaN/infinity checks
   - Range validation

### 15.4 Optimization Opportunities

#### 15.4.1 Precision Tuning

The most effective optimization is reducing MPFR precision:

- 53-bit (double): Baseline performance
- 128-bit: ~2x slower than 53-bit
- 256-bit: ~4x slower than 53-bit
- 512-bit: ~8x slower than 53-bit

#### 15.4.2 Batch Generation

Generating numbers in batches amortizes overhead:

```r
# Inefficient: one at a time
for(i in 1:n) { x[i] <- generatePRNG(1) }

# Efficient: batch generation
x <- generatePRNG(n)
```

#### 15.4.3 Disabling Optional Features

When cryptographic security isn't required:

```r
config <- list(
  use_crypto_mixing = FALSE,    # Disable crypto mixing
  use_threading = FALSE,        # Disable thread safety if single-threaded
  buffer_size = 100000         # Larger buffer for better performance
)
```

### 15.5 Performance vs. Security Trade-off

The 50x performance penalty is a deliberate design choice that provides:

1. **Cryptographic-grade randomness**: Suitable for security applications
2. **Arbitrary precision control**: Can generate numbers with thousands of bits of precision
3. **Mathematical rigor**: Based on proven chaotic dynamical systems
4. **Enhanced unpredictability**: Multiple layers of randomization

For applications where these properties are valuable, the performance cost is justified. For general statistical sampling, standard generators remain more appropriate.

### 15.6 Future Performance Improvements

Potential optimizations that could be explored:

1. **SIMD Parallelization**: Process multiple QI sequences in parallel using vector instructions
2. **GPU Acceleration**: Massive parallelism for batch generation
3. **Fixed-point Arithmetic**: For specific precision requirements, fixed-point could be faster
4. **JIT Compilation**: Compile parameter-specific optimized code at runtime
5. **Hardware PRNG Integration**: Use hardware random sources for mixing instead of libsodium

## 16. Statistical Test Algorithms

### 16.1 Gap Test

The gap test examines the distribution of gaps between occurrences of values within a specified range. For a truly random sequence, these gaps should follow a geometric distribution.

#### 16.1.1 Algorithm

1. **Define target range**: Default is [Î±, Î²] = [0.3, 0.7]
2. **Identify positions**: Find all indices where values fall within [Î±, Î²]
3. **Calculate gaps**: Compute differences between consecutive positions
4. **Expected distribution**: For uniform random data, gaps follow a geometric distribution with parameter p = Î² - Î±

#### 16.1.2 Mathematical Foundation (Rigorous Proof)

**Setup**: Let (Uâ‚™)â‚™â‰¥â‚ be i.i.d. uniform on [0,1]. Fix an interval [Î±, Î²] âŠ‚ [0,1] and set:

```
p := P(Uâ‚™ âˆˆ [Î±, Î²]) = Î² - Î±
```

Define indicator variables Xâ‚™ := ğŸ™{Uâ‚™ âˆˆ [Î±, Î²]} for n = 1, 2, .... Then (Xâ‚™) are i.i.d. Bernoulli(p).

Let Tâ‚ < Tâ‚‚ < ... be the (random) indices of hits:
- Tâ‚ := min{n â‰¥ 1 : Xâ‚™ = 1}
- T_{k+1} := min{n > Tâ‚– : Xâ‚™ = 1}

Define the gap lengths Gâ‚– := T_{k+1} - Tâ‚–.

**Theorem (Gap Distribution)**: For an i.i.d. uniform sequence on [0,1], the gaps between visits to a fixed interval [Î±, Î²] follow a geometric distribution:

```
P(G = k) = (1-p)^(k-1) Â· p,    p = Î² - Î±
```

**Proof**: We compute P(Gâ‚ = k) for k = 1, 2, ....

- Gâ‚ = 1 iff we see a hit immediately after Tâ‚: that is X_{Tâ‚+1} = 1
- In general, Gâ‚ = k means: after a hit at Tâ‚, we see k-1 misses followed by a hit

Because the sequence is i.i.d., conditioning on the fact that time Tâ‚ is a hit, the sequence (X_{Tâ‚+1}, X_{Tâ‚+2}, ...) is again i.i.d. Bernoulli(p) and independent of the past. Thus:

```
P(Gâ‚ = k) = P(X_{Tâ‚+1} = 0, ..., X_{Tâ‚+k-1} = 0, X_{Tâ‚+k} = 1)
          = (1-p)^(k-1) Â· p
```

By the same argument, the post-Tâ‚‚ process is independent and identically distributed to the original, so each gap Gâ‚– has the same law. âˆ

#### 16.1.3 Expected Frequencies and Ï‡Â² Test

If you observe N_gaps gaps and bucket them as lengths 1, 2, ..., K-1 and "â‰¥ K", then:

- For 1 â‰¤ k â‰¤ K-1:
  ```
  E[count of gaps with length k] = N_gaps Â· (1-p)^(k-1) Â· p
  ```

- For the last bin "â‰¥ K":
  ```
  E[count of gaps with length â‰¥ K] = N_gaps Â· Î£_{kâ‰¥K} (1-p)^(k-1) Â· p = N_gaps Â· (1-p)^(K-1)
  ```

The Ï‡Â² statistic is:

```
Ï‡Â² = Î£_{k=1}^{K} (Oâ‚– - Eâ‚–)Â² / Eâ‚–
```

Under the null (true geometric with parameter p), and assuming all Eâ‚– are reasonably large, Ï‡Â² is approximately Ï‡Â²_{K-1-m} distributed, where m is the number of estimated parameters (often m=1 if you estimate pÌ‚ from data).

#### 16.1.4 Implementation Details

```r
# Expected frequencies for gaps of length 1, 2, ..., max_gap
expected_freq[k] = n_gaps * (1-p)^(k-1) * p

# For the last bin (gaps â‰¥ max_gap)
expected_freq[max_gap] = n_gaps * (1-p)^(max_gap-1)

# Chi-squared statistic
Ï‡Â² = Î£((observed[k] - expected[k])Â²/expected[k])
```

### 16.2 Spectral Test

The spectral test analyzes the frequency domain characteristics of the random sequence using the periodogram and tests whether the spectral densities follow the expected distribution for white noise.

#### 16.2.1 Algorithm

1. **Compute periodogram**: Use Fast Fourier Transform (FFT) to calculate power spectral density
2. **Normalize spectrum**: Scale spectral values by their mean
3. **Expected distribution**: For white noise, normalized spectral densities follow an exponential distribution with rate parameter 1
4. **Test goodness-of-fit**: Use Kolmogorov-Smirnov test to compare with exponential(1) distribution

#### 16.2.2 Mathematical Foundation (Rigorous Proof)

**Setup**: Let (Xâ‚œ)_{t=0}^{N-1} be i.i.d. with mean 0 and variance ÏƒÂ². For spectral arguments, assume either:
- Xâ‚œ is Gaussian, or
- Xâ‚œ is a general i.i.d. sequence with finite second moment (CLT applies asymptotically)

Define the discrete Fourier transform (DFT):

```
XÌƒ(Ï‰â‚–) := Î£_{t=0}^{N-1} Xâ‚œ exp(-2Ï€ikt/N),    k = 0, 1, ..., N-1
```

The (raw) periodogram at frequency Ï‰â‚– is:

```
I(Ï‰â‚–) := (1/2Ï€N) |XÌƒ(Ï‰â‚–)|Â²
```

For white noise, the true spectral density is constant: f(Ï‰) â‰¡ ÏƒÂ²/2Ï€.

We focus on 1 â‰¤ k â‰¤ N/2-1, the "interior" frequencies (excluding 0 and Nyquist).

**Distribution of the DFT under Gaussian white noise**:

Assume Xâ‚œ ~ N(0, ÏƒÂ²) i.i.d. Each XÌƒ(Ï‰â‚–) is a linear combination of Gaussian variables, hence Gaussian. For 1 â‰¤ k â‰¤ N/2-1, the real and imaginary parts are (asymptotically) independent, each with variance NÏƒÂ²/2.

More explicitly:
```
Re(XÌƒ(Ï‰â‚–)) = Î£_{t=0}^{N-1} Xâ‚œ cos(2Ï€kt/N)
Im(XÌƒ(Ï‰â‚–)) = -Î£_{t=0}^{N-1} Xâ‚œ sin(2Ï€kt/N)
```

By independence and zero mean:
```
Var(Re(XÌƒ(Ï‰â‚–))) = Î£_{t=0}^{N-1} ÏƒÂ² cosÂ²(2Ï€kt/N) â‰ˆ NÏƒÂ²/2
```
(similarly for the imaginary part).

Thus, for large N:
```
Re(XÌƒ(Ï‰â‚–)), Im(XÌƒ(Ï‰â‚–)) â‰ˆ N(0, NÏƒÂ²/2), independent
```

So we can write approximately:
```
XÌƒ(Ï‰â‚–) â‰ˆ âˆš(NÏƒÂ²/2) (Zâ‚ + iZâ‚‚)
```
with Zâ‚, Zâ‚‚ i.i.d. N(0,1).

Then:
```
|XÌƒ(Ï‰â‚–)|Â² = (NÏƒÂ²/2)(Zâ‚Â² + Zâ‚‚Â²)
```

Hence the periodogram:
```
I(Ï‰â‚–) = (1/2Ï€N)|XÌƒ(Ï‰â‚–)|Â² â‰ˆ (1/2Ï€N) Â· (NÏƒÂ²/2)(Zâ‚Â² + Zâ‚‚Â²) = (ÏƒÂ²/4Ï€)(Zâ‚Â² + Zâ‚‚Â²)
```

But Zâ‚Â² + Zâ‚‚Â² has a Ï‡Â²â‚‚ distribution, and Ï‡Â²â‚‚ ~ 2Â·Exp(1). So:
```
Zâ‚Â² + Zâ‚‚Â² = 2E,    E ~ Exp(1)
```

Thus:
```
I(Ï‰â‚–) â‰ˆ (ÏƒÂ²/4Ï€) Â· 2E = (ÏƒÂ²/2Ï€) Â· E
```

Recall the true spectral density f(Ï‰â‚–) = ÏƒÂ²/2Ï€. Therefore:

**Theorem (Spectral Ordinates)**: For i.i.d. white noise Xâ‚œ with variance ÏƒÂ², the normalized periodogram ordinates satisfy:
```
I(Ï‰â‚–)/f(Ï‰â‚–) â‰ˆ E ~ Exp(1)
```

#### 16.2.3 Normalizing by the Sample Mean

In practice, we normalize by the empirical mean of the periodogram ordinates:
```
Äª := (1/M) Î£_{k=1}^{M} I(Ï‰â‚–)
```

and form:
```
Sâ‚– := I(Ï‰â‚–)/Äª
```

For large M, the law of large numbers implies Äª â†’ f(Ï‰) almost surely, so Sâ‚– converges in distribution to Exp(1):
```
Sâ‚– â‡’ Exp(1)
```

This justifies the KS test against the exponential distribution.

#### 16.2.4 Implementation Details

```r
# Compute spectrum using R's spectrum function
spec_result <- spectrum(x, method = "pgram", plot = FALSE)

# Normalize by mean
scaled_spec <- spec_result$spec / mean(spec_result$spec)

# KS test against exponential(1)
ks_result <- ks.test(scaled_spec, "pexp", rate = 1)
```

The Kolmogorov-Smirnov test compares the empirical CDF of the {Sâ‚–} to the theoretical CDF F(x) = 1 - e^(-x) of Exp(1).

#### 16.2.5 Interpretation

- **High p-value**: Spectral characteristics consistent with white noise
- **Low p-value**: Presence of periodic patterns or non-random structure
- **Peak-to-mean ratio**: Additional diagnostic showing the strength of any dominant frequency
