# Technical Documentation for the Quadratic Irrational PRNG v0.7.3

**Attribution Note**: The mathematical foundation and core algorithm design presented in this document is based on the work of Vincent Granville (2022), specifically his research on random number generators using quadratic irrationals. While Granville provided the original Python implementation, this document describes an R/C++ implementation developed primarily for educational and research purposes. Note: The original work has not been peer-reviewed in cryptographic literature.

**Reference**: Granville, V. (2022). Synthetic Data and Generative AI 1st Edition. <https://mltechniques.com/2022/12/13/military-grade-fast-random-number-generator-based-on-quadratic-irrationals/>

### Advanced Mixing Strategies

Five mixing strategies combine outputs from multiple QI generators:

1. **Round Robin**: Sequential cycling through generators
2. **XOR Mixing**: Bitwise XOR of mantissas for maximum entropy
3. **Averaging**: Weighted average for smooth distribution
4. **Modular Addition**: Sum modulo 1 for entropy combining
5. **Cascade Mixing**: Three-pass mixing (XOR â†’ modular â†’ nonlinear)

### SIMD Vectorization

Hardware-optimized operations for the **double-precision backend** (not MPFR arithmetic):

- **AVX2/AVX512**: 4-8 doubles per operation on x86
- **ARM NEON**: 2 doubles per operation on Apple Silicon
- **Automatic fallback**: Scalar operations when SIMD unavailable

**Note**: SIMD acceleration applies to mixing operations, batch transforms, and the optional double-precision computation path. The MPFR arbitrary-precision backend uses scalar operations and does not benefit from SIMD vectorization.

### Jump-Ahead and Parallel Stream Generation

1. **Parameter splitting**: Independent (a, b, c) triplets per stream (primary approach)
2. **Stream leapfrogging**: Deterministic subsequence partitioning
3. **Crypto counter-mode**: O(1) random access via ChaCha20 mixing

**Important note**: The 3Ã—3 matrix representation shown below is valid for single-step evaluation but does **not** provide O(log n) jump-ahead for the nonlinear quadratic map (see Section 4 for detailed analysis):

```cpp
// Single-step evaluation (NOT iterable via M^n)
[ x_{n+1} ]   [ a  b  c ] [ x_n^2 ]
[ x_n     ] = [ 1  0  0 ] [ x_n   ]
[ 1       ]   [ 0  0  1 ] [ 1     ]
```

For parallel applications, parameter splitting combined with crypto mixing provides the recommended approach.

## 1. Mathematical Foundation: The Quadratic Map

### 1.1 Basic Principle

The core algorithm, as described by Granville (2022), is based on a quadratic recurrence relation of the form:

```
x_{n+1} = (a*x_n^2 + b*x_n + c) mod 1
```

Where:

- `x_n` is the current state (a value in [0,1])
- `a`, `b`, and `c` are integer parameters
- `mod 1` means we take the fractional part of the result

This is a discrete dynamical system derived from the theory of quadratic irrationals. When properly configured, this map exhibits chaotic behavior, making it suitable for random number generation.

### 1.2 The Discriminant Requirement

The discriminant of the quadratic equation is defined as:

```
Î” = bÂ² - 4ac
```

**What Î” > 0 guarantees:**

If Î” > 0, then the polynomial q(x) = axÂ² + bx + c has two distinct real roots, hence the parabola crosses the real axis twice. This is a standard result from the quadratic formula.

**What Î” > 0 does NOT guarantee:**

The positive discriminant condition is a convenient parameter-screening heuristic that prevents degenerate polynomial shapes and correlates empirically with better orbit behavior for our parameter families. However, it is **not** a universal theorem guaranteeing:

- Absence of fixed points or short cycles on [0,1)
- Topological mixing or chaos for f(x) = {q(x)}
- An invariant distribution equal to uniform on [0,1)

**Why we use this requirement:**

1. **Empirical correlation**: Parameters with Î” > 0 empirically exhibit better statistical properties in our testing framework.

2. **Geometric interpretation**: When Î” > 0, the quadratic map has richer geometric behavior due to the parabola's interaction with the unit interval.

3. **Historical precedent**: This constraint follows Granville's original formulation and has been validated through extensive empirical testing.

**Important caveat**: The combination of a > 0, c < 0, and Î” > 0 does not mathematically rule out fixed points (see Section 9.4 and Appendix A for detailed analysis).

### 1.3 Orbit Structure

The sequence generated by this recurrence relation creates what mathematicians call an "orbit" in dynamical systems theory. With proper parameters, this orbit:

- Densely fills the interval [0,1]
- Never repeats exactly (with sufficient precision)
- Has a uniform distribution of points
- Exhibits exponential divergence of nearby starting values

## 2. Parameter Constraints and Their Reasoning

### 2.1 Parameter `a`

**Constraint**: `a > 0` (must be positive)

**Reasoning**:

- Controls the curvature and "stretching" factor of the quadratic map
- Positive values produce upward-opening parabolas that empirically correlate with richer folding behavior under mod-1 projection
- If zero, would degenerate into a linear map with reduced complexity

**Note**: This constraint is part of an empirical parameter family that tends to produce better statistical properties in our testing. Negative `a` does not automatically eliminate complex dynamics, but changes the geometry in ways our parameter search has not explored.

### 2.2 Parameter `c`

**Constraint**: `c < 0` (must be negative)

**Reasoning**:

- Together with `a > 0`, produces polynomial shapes that empirically exhibit better orbit behavior in our parameter families
- The mod-1 operation always maps results into [0,1) regardless of parameter signs

**Note**: The `c < 0` constraint correlates with "better behavior" in our searched families but does not enforce any topological guarantee. The fractional part operation handles range normalization independent of parameter values.

### 2.3 Discriminant: `bÂ² - 4ac > 0`

**Reasoning**:

- Ensures two distinct real roots for the polynomial q(x) = axÂ² + bx + c in â„, which correlates with stronger curvature/folding patterns before mod-1 projection
- Correlates empirically with better statistical properties for our parameter families
- Prevents degenerate polynomial shapes (single/no real roots) that tend to produce poor orbits

**Important limitations** (see Appendix A for formal analysis):

- Î” > 0 alone does **not** guarantee chaos, uniformity, or absence of fixed points
- Under mod-1 projection, these â„-geometric features do not translate into guaranteed mixing
- The chaotic behavior depends on the specific parameter values, not just the discriminant sign
- Empirical testing remains necessary to validate statistical quality for each parameter set

We restrict to a > 0, c < 0, and Î” > 0 as an empirical parameter family that tends to produce richer folding behavior under the mod-1 projection in our testing. These constraints are **not sufficient** to guarantee chaos, mixing, or the absence of short cycles.

### 2.4 Parameter Selection Guidelines

For optimal randomness properties:

1. **Coprime values**: Ideally, parameters `a`, `b`, and `c` should be coprime (no common divisors)
2. **Moderate values**: While any valid values work mathematically, moderate values (e.g., 1-10) tend to produce better orbit structure quality and avoid extremely large intermediate values in double-precision backends
3. **High precision**: Higher MPFR precision increases the effective state space, extending the period before finite-precision cycling occurs

## 3. MultiQI Implementation (v0.5.0 Enhanced)

### 3.1 Mixing Strategies

Version 0.5.0 introduces configurable mixing strategies for combining multiple QI sequences:

1. **Round-Robin** (Default):
   - Cycles through sequences sequentially
   - Lowest computational overhead
   - Good for general-purpose applications

2. **XOR Mixing**:
   - Combines outputs via bitwise XOR of mantissas
   - Good bit diffusion properties
   - Useful for applications requiring thorough mixing

3. **Averaging**:
   - Averages outputs from all QI sequences
   - Smooths distribution characteristics
   - Reduces individual sequence biases

4. **Modular Addition**:
   - Adds outputs modulo 1
   - Preserves uniform distribution
   - Good entropy mixing

5. **Cascade Mixing**:
   - Feeds output of one QI as input to next
   - Maximum entropy diffusion
   - Best mixing quality but slower performance

### 3.2 Mathematical Benefits

Each mixing strategy provides different theoretical advantages:

- **Period extension**: Mixing multiple streams can increase the effective cycle length in practice and empirically reduces detectable structure. However, no general closed-form period guarantee (such as LCM) is asserted for nonlinear mixed constructions under finite precision
- **Entropy amplification**: Multiple sources increase unpredictability
- **Bias reduction**: Averaging reduces systematic biases (note: averaging itself produces a triangular, not uniform, distributionâ€”see Section 7.2.2)
- **Correlation breaking**: XOR destroys linear correlations when at least one operand is uniform

### 3.2 Parameter Diversification

The implementation uses a carefully selected set of (a,b,c) parameters:

1. **Parameter selection**:
   - Parameters are chosen to have positive discriminants
   - Different parameters create sequences with different orbital characteristics
   - Parameter selection affects the "mixing speed" of the sequence

2. **Random skipping**:
   - Each QI sequence is initialized and then skipped a random number of steps
   - This ensures different starting points even with the same parameters
   - Prevents potential synchronization of sequences

3. **Manual Multi-QI Configuration** (v0.5.0):
   - Users can specify multiple QI parameters directly via vectors
   - Example: `a = c(2, 3, 5), b = c(7, 11, 13), c = c(-3, -5, -7)`
   - Each triplet (a[i], b[i], c[i]) defines a separate QI generator
   - All discriminants are validated to ensure mathematical correctness
   - Supports 2-16 simultaneous QI generators for optimal performance

### 3.3 Custom Discriminants via CSV

The implementation supports using custom discriminants from a CSV file:

1. **Mathematical advantages of custom discriminants**:
   - **Optimized orbital characteristics**: Carefully chosen discriminants can produce sequences with improved statistical properties
   - **Controlled cycle lengths**: Different discriminants lead to different cycle lengths, allowing optimization for specific applications
   - **Spectral properties**: The discriminant value directly influences the spectral properties of the sequence, with certain values producing better results in frequency domain tests
   - **Greater unpredictability**: Custom discriminants can enhance the unpredictability of the sequence, making it harder to reverse-engineer

2. **Selection criteria for optimal discriminants**:
   - **Prime or prime-rich factorizations**
   - **Congruence classes**
   - **Large discriminants**
   - **Spectral testing**: Discriminants that perform well on spectral tests minimize periodic patterns

3. **Implementation advantages**:
   - **Customization**: Users can tune the PRNG to specific application requirements
   - **Reproducibility**: Custom discriminants allow for reproducible sequences across different environments
   - **Parallel independence**: Using different discriminants for parallel streams ensures statistical independence
   - **Domain-specific optimization**: Applications with special requirements can benefit from discriminants optimized for those needs

4. **Usage example**:

   ```
   # Content of discriminants.csv
   a,b,c,Discriminant
   1,9,-143,653
   1,9,-145,661
   1,9,-148,673
   1,9,-149,677
   1,11,-145,701
   ...
   ```

   The CSV file provides complete (a,b,c) triplets along with their calculated discriminant values (bÂ² - 4ac). This format allows for direct specification of parameters known to have desirable properties, rather than deriving them from discriminants alone.

By default the package uses a curated set of 370 discriminants that passed comprehensive statistical testing.

## 4. Jump-Ahead Strategies (v0.5.0+)

### 4.1 The Nonlinearity Challenge

Given the quadratic recurrence:

```
x_{n+1} = (aÂ·x_nÂ² + bÂ·x_n + c) mod 1
```

A natural question is whether matrix exponentiation can provide O(log n) jump-ahead, as it does for linear congruential generators.

**Critical observation**: The 3Ã—3 matrix representation

```
[ x_{n+1}  ]   [ a  b  c ] [ x_nÂ²  ]
[ x_n      ] = [ 1  0  0 ] [ x_n   ]
[ 1        ]   [ 0  0  1 ] [ 1     ]
```

is **correct for one step** as an evaluation trick, but it does **not** imply that Mâ¿ performs an n-step jump.

**Why Mâ¿ does not encode iteration**: Let v_n := (x_nÂ², x_n, 1)áµ€. Even if we write x_{n+1} = [a b c]v_n, there is no fixed 3Ã—3 matrix M such that v_{n+1} = MÂ·v_n for all x_n, because:

```
x_{n+1}Â² = (ax_nÂ² + bx_n + c)Â²
```

is a **degree-4 polynomial** in x_n, hence cannot be expressed as a linear combination of (x_nÂ², x_n, 1) with constant coefficients.

**Proof**: If v_{n+1} = MÂ·v_n held with constant M, the first component of v_{n+1} would be linear in x_nÂ², x_n, 1â€”i.e., at most quadratic in x_n. But x_{n+1}Â² is quartic in x_n unless a = 0 (degenerate case). Contradiction. âˆ

### 4.2 True Jump-Ahead for Nonlinear Maps

For nonlinear maps, computing fâˆ˜â¿(x) requires either:

1. **Sequential iteration**: Apply f exactly n times (O(n) complexity)
2. **Function composition**: Compose f with itself symbolically (degree doubles each step, impractical for large n)
3. **Approximation methods**: Various numerical techniques with bounded accuracy

### 4.3 Practical Jump-Ahead Alternatives

The implementation provides several practical approaches for parallel stream generation:

#### 4.3.1 Stream Splitting / Leapfrogging

Thread i uses x_{i+mÂ·t} for stride m:

- Requires sequential generation to establish starting points
- Guarantees deterministic partitioning
- Each stream is a mathematically well-defined subsequence

#### 4.3.2 Parameter Splitting

Use independent discriminants/parameter triplets per stream:

- Different (a, b, c) values produce statistically independent sequences
- No need for jump-ahead computation
- This is the current primary approach for parallel generation

#### 4.3.3 Crypto Counter-Mode Enhancement

Generate C_n from ChaCha20(counter=n), then output (x_n + C_n) mod 1:

- Provides O(1) random access for the crypto component
- Preserves uniformity by the modular addition theorem (see Section 7.2.1)
- The QI component provides additional entropy even with sequential generation

### 4.4 Implementation Notes

The package implements practical jump-ahead using a combination of:

1. **MPFR-based iteration**: Direct computation with high precision for moderate jumps
2. **Parameter diversification**: Multiple independent QI generators for parallel streams
3. **ChaCha20 mixing**: Counter-mode operation provides addressable random access when uniformity is the primary goal

### 4.5 Complexity Summary

| Method | Complexity | Practical Use |
|--------|------------|---------------|
| Sequential iteration | O(n) | Small to moderate jumps |
| Parameter splitting | O(1) setup | Parallel stream generation |
| Crypto counter-mode | O(1) per sample | Random access with uniformity guarantee |

For most parallel applications, parameter splitting combined with crypto mixing provides the best balance of correctness, performance, and statistical quality.

## 5. CFE Period Detection (v0.5.0)

### 5.1 Continued Fraction Expansion

For quadratic irrationals of the form:

```
Î± = (âˆšD + P) / Q
```

The continued fraction expansion is:

```
Î± = a_0 + 1/(a_1 + 1/(a_2 + 1/(...)))
```

### 5.2 Theoretical Foundation: Lagrange's Theorem

**Definition**: A *quadratic irrational* is a real number Î± that is a root of AxÂ² + Bx + C = 0 with integers A, B, C, A â‰  0, and discriminant D = BÂ² - 4AC not a perfect square.

**Theorem (Lagrange)**: A real number has an eventually periodic simple continued fraction expansion if and only if it is a quadratic irrational.

Therefore, numbers of the form Î± = (âˆšD + Pâ‚€)/Qâ‚€ (with integer D > 0 not a perfect square) are exactly in this class, and their CFE is ultimately periodic.

### 5.3 Gauss-Legendre Algorithm

The package implements the Gauss-Legendre algorithm for O(L) period detection:

1. **Initialization**: Start with Pâ‚€ = 0, Qâ‚€ = 1
2. **Iteration**:

   ```
   aâ‚™ = floor((âˆšD + Pâ‚™) / Qâ‚™)
   P_{n+1} = aâ‚™ Â· Qâ‚™ - Pâ‚™
   Q_{n+1} = (D - P_{n+1}Â²) / Qâ‚™
   ```

3. **Period detection**: When (Pâ‚™, Qâ‚™) repeats, period is found

### 5.4 Why the Algorithm Terminates (Proof of Periodicity)

**Theorem (Periodicity of CFE)**: For Î± = (âˆšD + Pâ‚€)/Qâ‚€ with integer D > 0 not a square, the recursion on (Pâ‚™, Qâ‚™) visits only finitely many integer states. Hence (Pâ‚™, Qâ‚™) must eventually repeat, and the resulting CFE [aâ‚€; aâ‚, aâ‚‚, ...] is eventually periodic.

**Proof**: The crucial observation is that (Pâ‚™, Qâ‚™) live in a finite set of integer pairs:

1. **Boundedness of Qâ‚™**: It can be shown that Qâ‚™ divides D - Pâ‚™Â², and the algorithm maintains Q_{n+1} = (D - P_{n+1}Â²)/Qâ‚™ as an integer. The values |Qâ‚™| are bounded by D.

2. **Boundedness of Pâ‚™**: By the definition of the algorithm, |Pâ‚™| â‰¤ âˆšD for all n â‰¥ 1.

3. **Pigeonhole argument**: Since there are only finitely many integer pairs (P, Q) with |P| â‰¤ âˆšD and |Q| â‰¤ D, by the pigeonhole principle some pair must repeat: (Pâ‚™, Qâ‚™) = (Pâ‚˜, Qâ‚˜) with n > m.

4. **Periodicity follows**: Once the state (Pâ‚™, Qâ‚™) repeats, the sequence of partial quotients aâ‚™ repeats from that point onward, establishing the period. âˆ

### 5.5 Complexity Analysis

- **Time complexity**: O(L) where L is the period length
- **Space complexity**: O(L) to store the period
- **Practical performance**: Periods typically < 1000 for discriminants < 10â¶
- **Asymptotic bound**: Known results in algebraic number theory establish L = O(âˆšD log D) for discriminant D (see, e.g., results related to the Brauer-Siegel theorem and class number bounds). This is an asymptotic heuristic; individual discriminants may have shorter or longer periods.

## 6. Dynamic QI Generation (v0.5.0)

### 6.1 Discriminant Pattern

The package uses discriminants of the form D = kÂ² + 1:

- **Often square-free**: Many values of kÂ² + 1 are square-free, but this is *not* guaranteed
- **Counterexample**: For k = 7, we have kÂ² + 1 = 50 = 2 Ã— 5Â², which is *not* square-free
- Produces well-distributed quadratic irrationals when square-free
- Efficient validation: check if D - 1 is a perfect square

**Important**: The implementation validates square-freeness and discards discriminants that fail this test. The kÂ² + 1 pattern is a convenient source of candidate discriminants, not a guarantee of square-freeness.

### 6.2 Square-Free Validation

Algorithm for checking square-free property:

```
is_square_free(D):
  for p in small_primes:
    if D % (p*p) == 0:
      return false
  return true
```

This validation is **required** for all discriminants, including those of the form kÂ² + 1.

### 6.3 Benefits

- **Quality assurance**: Square-free validation ensures only valid discriminants are used
- **Efficient generation**: O(âˆšD) validation time
- **Good coverage**: kÂ² + 1 pattern provides a rich source of candidate discriminants

## 7. ChaCha20 Output Mixing

### 7.1 Purpose and Motivation

While the quadratic irrational generator produces sequences with good statistical properties, adding ChaCha20 output mixing provides:

1. **Enhanced statistical quality**: Eliminates subtle patterns that might exist in the raw sequence
2. **State recovery resistance**: When the ChaCha20 key remains secret, makes reverse-engineering the generator state more difficult
3. **Additional entropy**: Combines QI output with ChaCha20 stream

**Important**: This mixing layer does not constitute a formally analyzed CSPRNG construction. The security benefits come primarily from ChaCha20 itself, not from the combination. For applications requiring proven cryptographic security, use established primitives directly.

### 7.2 Mixing Approaches

The package implements two different approaches for output mixing:

#### 7.2.1 Partial Modular Addition (Default)

```
x_mixed = (x_original + crypto_uniform) mod 1
```

Where:

- `x_original` is the raw value from the quadratic irrational generator
- `crypto_uniform` is a cryptographically uniform value in [0,1), derived from ChaCha20 bytes
- The result is taken modulo 1 to keep it in [0,1)

**Implementation note**: ChaCha20 provides a uniform bitstream. When mapped to IEEE-754 doubles using 53 random bits for the mantissa (divided by 2^53), the result is uniform over the representable grid of 2^53 values in [0,1). This is "exactly uniform" in the discrete sense, not continuous measure-theoretic uniform, but the distinction is immaterial for all practical purposes.

**Theorem (Crypto Mixing Preserves Uniformity)**: If U ~ Unif[0,1) and C ~ Unif[0,1) are independent, then (U + C) mod 1 ~ Unif[0,1).

**Proof**: Let V := (U + C) mod 1. We want to show V ~ Unif[0,1).

Take any measurable set A âŠ‚ [0,1). Then:

```
P(V âˆˆ A) = P((U + C) mod 1 âˆˆ A)
         = âˆ«âˆ« ğŸ™{(u+c) mod 1 âˆˆ A} f_U(u) f_C(c) du dc
```

Since U, C are independent uniforms: f_U(u) = f_C(c) = 1 on [0,1).

Use the fact that adding a fixed c modulo 1 is just a rotation of the circle:

For each fixed c, the map u â†¦ (u + c) mod 1 is a bijection of [0,1) onto itself that preserves Lebesgue measure. Therefore, for uniform u:

```
P((U + c) mod 1 âˆˆ A) = |A|
```

where |A| is the Lebesgue measure of A.

Thus:

```
P(V âˆˆ A) = âˆ«â‚€Â¹ P((U + c) mod 1 âˆˆ A) dc = âˆ«â‚€Â¹ |A| dc = |A|
```

But P(V âˆˆ A) = |A| is exactly the defining property of Unif[0,1). âˆ

**Corollary**: Even if X_original is only approximately uniform, the mixing operation produces a uniform output (on the representable grid), since the ChaCha20-derived component C is cryptographically uniform.

**Benefits**:

- **Mathematically proven** to preserve the uniform distribution
- Uses ChaCha20 stream cipher for the mixing component
- Efficiently eliminates patterns in the original sequence

#### 7.2.2 Partial Averaging (Alternative)

An alternative approach that uses weighted averaging instead of modular addition, activated with the `adhoc_corrections` flag:

```
x_mixed = (x_original + crypto_uniform) / 2
```

**Important Mathematical Note**: Unlike modular addition, averaging does **not** preserve uniform distribution. If U, C ~ Unif[0,1) are independent, then (U + C)/2 has a triangular distribution on [0,1), not a uniform distribution. The density is:

```
f(x) = 4x       for 0 â‰¤ x â‰¤ 1/2
f(x) = 4(1-x)   for 1/2 < x â‰¤ 1
```

**Differences**:

- Produces a non-uniform (triangular) distribution concentrated toward 0.5
- May be useful for specific applications requiring central tendency
- Included as an alternative mixing strategy for testing purposes

### 7.3 Integration with Core Generator

The ChaCha20 output mixing integrates with the core generator in several ways:

1. **Buffer-based processing**:
   - Values are generated in buffers for efficiency
   - Entire buffers are cryptographically processed at once

2. **Reseeding mechanism**:
   - Both the quadratic parameters and the encryption key are replaced during reseeding
   - Reseeding can occur automatically after a specified number of generated values

3. **Careful handling of sensitive data**:
   - Encryption keys are stored in secure memory
   - Keys are properly zeroed when no longer needed using sodium_memzero
   - Side-channel mitigation techniques are employed

### 7.4 Tie-Breaking Mechanism (Optional)

The implementation includes an optional tie-breaking mechanism to handle duplicate consecutive values:

1. **Detection**:
   - Checks if the current output exactly equals the previous output
   - Such exact duplications are rare but statistically expected in any random sequence

2. **Resolution**:
   - Adds a tiny random epsilon value to break the tie
   - Ensures consecutive identical values never occur when enabled

3. **Important Limitations**:
   - **Distribution artifacts**: Introduces a state-dependent perturbation that may create subtle biases in ULP (unit in last place) space
   - **Reproducibility impact**: Unless epsilon is derived deterministically, strict reproducibility is lost
   - **Statistically unnecessary**: True random sequences do produce duplicate values; consecutive duplicates are rare but valid

4. **Recommendation**:
   - For most applications, duplicates should be allowed (the mathematically correct behavior)
   - Only enable tie-breaking if your downstream application has a hard requirement for unique consecutive values
   - If enabled, be aware this is a nonstandard modification that may affect certain statistical tests

## 8. Distribution Transformations

The raw uniform [0,1] values can be transformed into various probability distributions:

### 8.1 Uniform Range

Transforms uniform [0,1] to uniform [min, max]:

```
y = min + x*(max - min)
```

### 8.2 Normal Distribution

Uses the Box-Muller transform to generate normally distributed values:

```
r = sqrt(-2*ln(u1))
Î¸ = 2Ï€*u2
x = mean + sd * r*cos(Î¸)
y = mean + sd * r*sin(Î¸)
```

Where u1 and u2 are independent uniform [0,1] values. The implementation optimizes this by storing the second value (y) for the next call, effectively using both random numbers generated from each pair of uniform draws.

### 8.3 Exponential Distribution

Uses the inverse transform method:

```
x = -ln(1-u)/Î»
```

Where u is a uniform [0,1] value and Î» is the rate parameter.

## 9. Precision and Numerical Considerations

### 9.1 MPFR Precision

The package uses the GNU MPFR library for arbitrary-precision arithmetic:

- **Default**: 53 bits (equivalent to double precision)
- **Range**: 24 to 10,000 bits (package limit)
- **Impact**: Higher precision increases cycle length and improves statistical properties

### 9.2 Numerical Stability

Considerations for numerical stability:

- **Overflow protection**: All calculations use arbitrary precision to prevent overflow
- **Underflow handling**: Special handling for values close to zero
- **Modular reduction**: Exact fractional part calculation ensures values stay in [0,1)

### 9.3 Period Length

With sufficient precision, the period length of the generator is extremely long:

- At 53-bit precision: effectively non-repeating for practical applications
- At higher precisions: period grows exponentially
- With ChaCha20 mixing: effective period becomes very large

### 9.4 Fixed Point Avoidance

A critical consideration in the implementation is avoiding fixed points. A fixed point is a value x where f(x) = x, meaning the sequence would get stuck at this value.

**Issues with Fixed Points:**

- Early implementations using the fractional part of 1/(ax + b) could converge to fixed points (e.g., 0.1861407...)
- Once convergence occurs, random numbers are no longer generated, as all subsequent values are identical
- This severely impacts the statistical properties and utility of the PRNG

**Fixed Point Analysis:**

For the map f(x) = {axÂ² + bx + c} (where {Â·} denotes fractional part), a fixed point in [0,1) is any x and integer k such that:

```
axÂ² + bx + c = x + k  âŸº  axÂ² + (b-1)x + (c-k) = 0
```

with x âˆˆ [0,1). For given integers a > 0, b, c < 0, nothing prevents some integer k from making this quadratic have a root in [0,1). **The constraints a > 0, c < 0, Î” > 0 alone do not mathematically guarantee the absence of fixed points.**

**Practical Mitigation:**

1. The current implementation uses the full quadratic map (axÂ² + bx + c mod 1)
2. For our selected parameter sets, empirical testing shows no convergence to fixed points
3. Cryptographic mixing provides strong protection: even if a fixed point were approached, the mixing would immediately move the state away
4. Random initial values and periodic reseeding further prevent any potential fixed point convergence
5. Fully ruling out fixed points for all parameter sets would require a separate, more technical argument specific to each parameter choice

**Note**: The combination of empirical validation, ChaCha20 output mixing, and reseeding makes fixed point convergence practically impossible, even though we do not have a blanket mathematical proof for all parameter combinations.

## 10. Thread Safety and Parallelism

### 10.1 Thread Safety Mechanisms

The package provides two approaches to thread safety:

1. **Single-Thread Mode** (`use_threading = FALSE`, default):
   - Optimized for single-threaded use with no synchronization overhead
   - If called from multiple threads without external synchronization, behavior is undefined
   - Guarantees sequence reproducibility in single-threaded contexts

2. **Multi-Thread Mode** (`use_threading = TRUE`):
   - Each thread gets its own independent PRNG instance via thread-local storage
   - No mutex contention, allowing full parallelism
   - Different random sequences in each thread (seeded independently)
   - Use this mode when generating random numbers from multiple threads concurrently

### 10.2 Mathematical Implications of Threading

Using thread-local PRNG instances (multi-thread mode) has important mathematical implications:

1. **Independence**:
   - Sequences generated in different threads are statistically independent
   - No correlation between sequences across threads
   - Reduces potential for pattern emergence in parallel applications

2. **Reproducibility**:
   - Single-thread mode: sequence is fully reproducible given the same seed
   - Multi-thread mode: each thread's sequence is reproducible given its seed, but the global interleaving depends on thread scheduling
   - Important consideration for scientific computing and simulation

3. **Performance vs. Determinism**:
   - Multi-thread mode offers better performance for concurrent workloads
   - Single-thread mode offers fully deterministic results
   - Choice depends on application requirements

## 11. Secure Memory Management

### 11.1 SecureBuffer Implementation

The package uses a template class called `SecureBuffer` for secure memory management:

1. **RAII Design**:
   - Resource Acquisition Is Initialization pattern
   - Automatically handles proper cleanup of sensitive data
   - Prevents memory leaks and security vulnerabilities

2. **Secure Memory Clearing**:
   - Overwrites memory with zeros before deallocation
   - Uses volatile pointers to prevent compiler optimization
   - Ensures sensitive data (like encryption keys) is securely erased

3. **Memory Safety**:
   - Type-safe wrapper around std::vector
   - Prevents buffer overflows and memory corruption
   - Simplifies memory management throughout the codebase

### 11.2 Side-Channel Mitigation

The implementation includes several measures to reduce side-channel attack vectors:

1. **Constant-time operations** where possible
2. **Avoidance of secret-dependent control flow**
3. **Protection against timing attacks**
4. **Use of libsodium's secure memory functions**

### 11.3 Application to Sensitive Data

SecureBuffer is applied to:

- Cryptographic keys
- Nonces
- Temporary buffers containing sensitive data
- Any other security-critical components

## 12. Implementation Note on Use of SecureBuffer

The qiprng implementation uses the SecureBuffer template class for secure memory management of sensitive data. This class:

1. Provides RAII-style memory management, automatically clearing sensitive data when it goes out of scope
2. Implements a secure clearing procedure that overwrites memory with zeros before deallocation
3. Ensures that cryptographic keys and other sensitive values are properly handled

When implementing or modifying code, it's important to:

- Always use SecureBuffer for any security-sensitive data
- Avoid creating unnecessary copies of sensitive data
- Allow the SecureBuffer destructor to handle clearing instead of manual clearing
- Remember that SecureBuffer constructor takes only a size parameter, not an initializer value

## 13. Memory Management Optimizations (v0.5.0)

### 13.1 MPFR Memory Pooling

Version 0.5.0 introduces a singleton memory pool for MPFR objects:

1. **Pool Architecture**:
   - Pre-allocated pool of 1024 MPFR objects
   - Lock-free allocation using atomic operations
   - Zero-copy returns to pool

2. **Performance Benefits**:
   - Eliminates malloc/free overhead
   - Reduces memory fragmentation
   - Improves cache locality

3. **Thread Safety**:
   - Thread-safe allocation/deallocation
   - Per-thread quotas prevent starvation
   - Fallback to heap allocation when pool exhausted

### 13.2 Thread-Local Storage

Each thread maintains its own MultiQI instance:

1. **ThreadManager Singleton**:
   - Manages thread-local QI instances
   - Automatic cleanup on thread exit
   - Prevents race conditions

2. **Benefits**:
   - No mutex contention for generation
   - Better cache performance
   - Scalable to many threads

3. **Memory Efficiency**:
   - Lazy initialization per thread
   - Automatic cleanup prevents leaks
   - Shared discriminant data

## 14. Discriminants (pre-selected)

The package includes a set of pre-selected discriminants for quadratic irrationals.

### Default Discriminant Selection

The package uses a curated set of 370 discriminants by default, selected through comprehensive statistical testing.

**Selection Criteria** (all must be satisfied):

- **Low autocorrelation**: `max_abs_acf â‰¤ 0.010`
- **No significant lags**: No autocorrelation beyond statistical thresholds
- **Multi-test validation**: Pass uniformity, independence, and periodicity tests

**Typical Parameter Patterns**:

- **Parameter `a`**: Small values (1-3) perform well
- **Discriminant range**: 1000-2000 shows consistently good quality
- **Parameter `c`**: Moderately negative values (-120 to -150) preferred

Each selected discriminant passed comprehensive statistical validation including autocorrelation, uniformity, independence, and periodicity tests with 1,000,000 sample sequences.

The package includes all 750 discriminants for research purposes, but uses the curated set by default unless explicitly configured otherwise.

### Discriminant Analysis Summary

A comprehensive analysis of all 750 discriminants was performed, each tested with 1,000,000 samples. The results confirm that while the underlying generation method is robust, rigorous testing is useful for identifying parameter sets with the best statistical properties.

- **370 curated discriminants**: 49.3% passed all tests with the best statistical properties
- **Overall robustness**: 92.5% of discriminants showed good or better performance, confirming the underlying strength of the generation method

## 15. Performance Analysis

### 15.1 Benchmarking Results

In comprehensive benchmarks comparing qiprng with standard R generators, the following performance characteristics were observed:

| Generator | Time for 100K samples | Relative Speed | Statistical Quality |
|-----------|----------------------|----------------|-------------------|
| qiprng | ~27ms | 1x (baseline) | Excellent |
| Mersenne Twister | ~0.5ms | 54x faster | Excellent |
| L'Ecuyer-CMRG | ~0.9ms | 30x faster | Very Good |
| Wichmann-Hill | ~0.4ms | 68x faster | Good |

### 15.2 Computational Complexity Analysis

The performance difference stems from fundamental algorithmic differences:

#### 15.2.1 Operation Count per Random Number

**Standard Linear Congruential Generators (e.g., Mersenne Twister)**:

- 1 integer multiplication
- 1 integer addition
- 1 modulo operation (often optimized away)
- Total: ~3 simple integer operations

**qiprng Quadratic Irrational Generator**:

- 1 MPFR squaring operation (mpfr_mul)
- 2 MPFR scalar multiplications (mpfr_mul_si)
- 2 MPFR additions (mpfr_add, mpfr_add_si)
- 1 MPFR fractional part extraction (mpfr_frac)
- 1 MPFR sign check and conditional addition
- Total: ~7 arbitrary precision operations

#### 15.2.2 MPFR Overhead

Each MPFR operation involves:

- **Dynamic memory management**: Arbitrary precision requires dynamic allocation
- **Software arithmetic**: No hardware acceleration for arbitrary precision
- **Rounding control**: Each operation must respect the rounding mode
- **Exception handling**: NaN and infinity checks after each operation

Typical overhead: 100-1000x slower than hardware floating-point operations

### 15.3 Component-wise Performance Breakdown

Based on code analysis and profiling estimates:

1. **MPFR Arithmetic Operations** (~60% of runtime)
   - Dominated by mpfr_mul for squaring
   - Fractional part extraction (mpfr_frac) is particularly expensive

2. **Cryptographic Mixing** (~20% when enabled)
   - libsodium ChaCha20 generation
   - Modular addition or XOR operations
   - Periodic reseeding

3. **Thread Safety** (~10%)
   - Mutex acquisition/release
   - Thread-local storage access
   - Memory barriers for synchronization

4. **Buffer Management** (~5%)
   - Buffer refill operations
   - Memory copying
   - Position tracking

5. **Error Checking & Validation** (~5%)
   - Parameter validation
   - NaN/infinity checks
   - Range validation

### 15.4 Optimization Opportunities

#### 15.4.1 Precision Tuning

The most effective optimization is reducing MPFR precision:

- 53-bit (double): Baseline performance
- 128-bit: ~2x slower than 53-bit
- 256-bit: ~4x slower than 53-bit
- 512-bit: ~8x slower than 53-bit

#### 15.4.2 Batch Generation

Generating numbers in batches amortizes overhead:

```r
# Inefficient: one at a time
for(i in 1:n) { x[i] <- generatePRNG(1) }

# Efficient: batch generation
x <- generatePRNG(n)
```

#### 15.4.3 Disabling Optional Features

When maximum speed is needed:

```r
config <- list(
  use_crypto_mixing = FALSE,    # Disable ChaCha20 mixing
  use_threading = FALSE,        # Disable thread safety if single-threaded
  buffer_size = 100000         # Larger buffer for better performance
)
```

### 15.5 Performance vs. Precision Trade-off

The 50x performance penalty compared to simple generators is a deliberate design choice that provides:

1. **Arbitrary precision control**: Can generate numbers with thousands of bits of precision
2. **Mathematical rigor**: Based on chaotic dynamical systems
3. **Exotic seeding mechanisms**: Flexible initialization options
4. **Enhanced statistical quality**: Multiple mixing strategies available

For applications where these properties are valuable, the performance cost may be justified. For general statistical sampling, standard generators remain more appropriate.

### 15.6 Future Performance Improvements

Potential optimizations that could be explored:

1. **SIMD Parallelization**: Process multiple QI sequences in parallel using vector instructions
2. **GPU Acceleration**: Massive parallelism for batch generation
3. **Fixed-point Arithmetic**: For specific precision requirements, fixed-point could be faster
4. **JIT Compilation**: Compile parameter-specific optimized code at runtime
5. **Hardware PRNG Integration**: Use hardware random sources for mixing instead of libsodium

## 16. Statistical Test Algorithms

### 16.1 Gap Test

The gap test examines the distribution of gaps between occurrences of values within a specified range. For a truly random sequence, these gaps should follow a geometric distribution.

#### 16.1.1 Algorithm

1. **Define target range**: Default is [Î±, Î²] = [0.3, 0.7]
2. **Identify positions**: Find all indices where values fall within [Î±, Î²]
3. **Calculate gaps**: Compute differences between consecutive positions
4. **Expected distribution**: For uniform random data, gaps follow a geometric distribution with parameter p = Î² - Î±

#### 16.1.2 Mathematical Foundation (Rigorous Proof)

**Setup**: Let (Uâ‚™)â‚™â‰¥â‚ be i.i.d. uniform on [0,1]. Fix an interval [Î±, Î²] âŠ‚ [0,1] and set:

```
p := P(Uâ‚™ âˆˆ [Î±, Î²]) = Î² - Î±
```

Define indicator variables Xâ‚™ := ğŸ™{Uâ‚™ âˆˆ [Î±, Î²]} for n = 1, 2, .... Then (Xâ‚™) are i.i.d. Bernoulli(p).

Let Tâ‚ < Tâ‚‚ < ... be the (random) indices of hits:

- Tâ‚ := min{n â‰¥ 1 : Xâ‚™ = 1}
- T_{k+1} := min{n > Tâ‚– : Xâ‚™ = 1}

Define the gap lengths Gâ‚– := T_{k+1} - Tâ‚–.

**Theorem (Gap Distribution)**: For an i.i.d. uniform sequence on [0,1], the gaps between visits to a fixed interval [Î±, Î²] follow a geometric distribution:

```
P(G = k) = (1-p)^(k-1) Â· p,    p = Î² - Î±
```

**Proof**: We compute P(Gâ‚ = k) for k = 1, 2, ....

- Gâ‚ = 1 iff we see a hit immediately after Tâ‚: that is X_{Tâ‚+1} = 1
- In general, Gâ‚ = k means: after a hit at Tâ‚, we see k-1 misses followed by a hit

Because the sequence is i.i.d., conditioning on the fact that time Tâ‚ is a hit, the sequence (X_{Tâ‚+1}, X_{Tâ‚+2}, ...) is again i.i.d. Bernoulli(p) and independent of the past. Thus:

```
P(Gâ‚ = k) = P(X_{Tâ‚+1} = 0, ..., X_{Tâ‚+k-1} = 0, X_{Tâ‚+k} = 1)
          = (1-p)^(k-1) Â· p
```

By the same argument, the post-Tâ‚‚ process is independent and identically distributed to the original, so each gap Gâ‚– has the same law. âˆ

#### 16.1.3 Expected Frequencies and Ï‡Â² Test

If you observe N_gaps gaps and bucket them as lengths 1, 2, ..., K-1 and "â‰¥ K", then:

- For 1 â‰¤ k â‰¤ K-1:

  ```
  E[count of gaps with length k] = N_gaps Â· (1-p)^(k-1) Â· p
  ```

- For the last bin "â‰¥ K":

  ```
  E[count of gaps with length â‰¥ K] = N_gaps Â· Î£_{kâ‰¥K} (1-p)^(k-1) Â· p = N_gaps Â· (1-p)^(K-1)
  ```

The Ï‡Â² statistic is:

```
Ï‡Â² = Î£_{k=1}^{K} (Oâ‚– - Eâ‚–)Â² / Eâ‚–
```

Under the null (true geometric with parameter p), and assuming all Eâ‚– are reasonably large, Ï‡Â² is approximately Ï‡Â²_{K-1-m} distributed, where m is the number of estimated parameters (often m=1 if you estimate pÌ‚ from data).

#### 16.1.4 Implementation Details

```r
# Expected frequencies for gaps of length 1, 2, ..., max_gap
expected_freq[k] = n_gaps * (1-p)^(k-1) * p

# For the last bin (gaps â‰¥ max_gap)
expected_freq[max_gap] = n_gaps * (1-p)^(max_gap-1)

# Chi-squared statistic
Ï‡Â² = Î£((observed[k] - expected[k])Â²/expected[k])
```

### 16.2 Spectral Test

The spectral test analyzes the frequency domain characteristics of the random sequence using the periodogram and tests whether the spectral densities follow the expected distribution for white noise.

#### 16.2.1 Algorithm

1. **Compute periodogram**: Use Fast Fourier Transform (FFT) to calculate power spectral density
2. **Normalize spectrum**: Scale spectral values by their mean
3. **Expected distribution**: For white noise, normalized spectral densities follow an exponential distribution with rate parameter 1
4. **Test goodness-of-fit**: Use Kolmogorov-Smirnov test to compare with exponential(1) distribution

#### 16.2.2 Mathematical Foundation (Rigorous Proof)

**Setup**: Let (Xâ‚œ)_{t=0}^{N-1} be i.i.d. with mean 0 and variance ÏƒÂ². For spectral arguments, assume either:

- Xâ‚œ is Gaussian, or
- Xâ‚œ is a general i.i.d. sequence with finite second moment (CLT applies asymptotically)

Define the discrete Fourier transform (DFT):

```
XÌƒ(Ï‰â‚–) := Î£_{t=0}^{N-1} Xâ‚œ exp(-2Ï€ikt/N),    k = 0, 1, ..., N-1
```

The (raw) periodogram at frequency Ï‰â‚– is:

```
I(Ï‰â‚–) := (1/2Ï€N) |XÌƒ(Ï‰â‚–)|Â²
```

For white noise, the true spectral density is constant: f(Ï‰) â‰¡ ÏƒÂ²/2Ï€.

We focus on 1 â‰¤ k â‰¤ N/2-1, the "interior" frequencies (excluding 0 and Nyquist).

**Distribution of the DFT under Gaussian white noise**:

Assume Xâ‚œ ~ N(0, ÏƒÂ²) i.i.d. Each XÌƒ(Ï‰â‚–) is a linear combination of Gaussian variables, hence Gaussian. For 1 â‰¤ k â‰¤ N/2-1, the real and imaginary parts are (asymptotically) independent, each with variance NÏƒÂ²/2.

More explicitly:

```
Re(XÌƒ(Ï‰â‚–)) = Î£_{t=0}^{N-1} Xâ‚œ cos(2Ï€kt/N)
Im(XÌƒ(Ï‰â‚–)) = -Î£_{t=0}^{N-1} Xâ‚œ sin(2Ï€kt/N)
```

By independence and zero mean:

```
Var(Re(XÌƒ(Ï‰â‚–))) = Î£_{t=0}^{N-1} ÏƒÂ² cosÂ²(2Ï€kt/N) â‰ˆ NÏƒÂ²/2
```

(similarly for the imaginary part).

Thus, for large N:

```
Re(XÌƒ(Ï‰â‚–)), Im(XÌƒ(Ï‰â‚–)) â‰ˆ N(0, NÏƒÂ²/2), independent
```

So we can write approximately:

```
XÌƒ(Ï‰â‚–) â‰ˆ âˆš(NÏƒÂ²/2) (Zâ‚ + iZâ‚‚)
```

with Zâ‚, Zâ‚‚ i.i.d. N(0,1).

Then:

```
|XÌƒ(Ï‰â‚–)|Â² = (NÏƒÂ²/2)(Zâ‚Â² + Zâ‚‚Â²)
```

Hence the periodogram:

```
I(Ï‰â‚–) = (1/2Ï€N)|XÌƒ(Ï‰â‚–)|Â² â‰ˆ (1/2Ï€N) Â· (NÏƒÂ²/2)(Zâ‚Â² + Zâ‚‚Â²) = (ÏƒÂ²/4Ï€)(Zâ‚Â² + Zâ‚‚Â²)
```

But Zâ‚Â² + Zâ‚‚Â² has a Ï‡Â²â‚‚ distribution, and Ï‡Â²â‚‚ ~ 2Â·Exp(1). So:

```
Zâ‚Â² + Zâ‚‚Â² = 2E,    E ~ Exp(1)
```

Thus:

```
I(Ï‰â‚–) â‰ˆ (ÏƒÂ²/4Ï€) Â· 2E = (ÏƒÂ²/2Ï€) Â· E
```

Recall the true spectral density f(Ï‰â‚–) = ÏƒÂ²/2Ï€. Therefore:

**Theorem (Spectral Ordinates)**: For i.i.d. white noise Xâ‚œ with variance ÏƒÂ², the normalized periodogram ordinates satisfy:

```
I(Ï‰â‚–)/f(Ï‰â‚–) â‰ˆ E ~ Exp(1)
```

#### 16.2.3 Normalizing by the Sample Mean

In practice, we normalize by the empirical mean of the periodogram ordinates:

```
Äª := (1/M) Î£_{k=1}^{M} I(Ï‰â‚–)
```

and form:

```
Sâ‚– := I(Ï‰â‚–)/Äª
```

For large M, the law of large numbers implies Äª â†’ f(Ï‰) almost surely, so Sâ‚– converges in distribution to Exp(1):

```
Sâ‚– â‡’ Exp(1)
```

This justifies the KS test against the exponential distribution.

#### 16.2.4 Implementation Details

```r
# Compute spectrum using R's spectrum function
spec_result <- spectrum(x, method = "pgram", plot = FALSE)

# Normalize by mean
scaled_spec <- spec_result$spec / mean(spec_result$spec)

# KS test against exponential(1)
ks_result <- ks.test(scaled_spec, "pexp", rate = 1)
```

The Kolmogorov-Smirnov test compares the empirical CDF of the {Sâ‚–} to the theoretical CDF F(x) = 1 - e^(-x) of Exp(1).

#### 16.2.5 Interpretation

- **High p-value**: Spectral characteristics consistent with white noise
- **Low p-value**: Presence of periodic patterns or non-random structure
- **Peak-to-mean ratio**: Additional diagnostic showing the strength of any dominant frequency

---

## Appendix A â€” Mathematical Hardening / Proof Notes

This appendix provides rigorous mathematical foundations for the QI PRNG, clarifying what is provable versus empirical, and correcting some claims that appeared in earlier documentation.

### A.0 Notation and Model (Formal)

Define the state space as the circle group:

```
ğ•‹ := â„/â„¤ â‰… [0,1)
```

with addition mod 1.

Let:

```
f(x) := {axÂ² + bx + c} âˆˆ [0,1)
```

where {y} := y - âŒŠyâŒ‹ is the fractional part and a, b, c âˆˆ â„¤ with a > 0.

The PRNG state evolves by:

```
x_{n+1} = f(x_n),    x_0 âˆˆ [0,1)
```

This is a deterministic dynamical system on ğ•‹. The generator outputs x_n (or transforms thereof).

**Important modeling distinction**: Mathematically, x_n âˆˆ ğ•‹ is a real point; computationally, x_n is represented with finite precision (e.g., MPFR with p bits), which turns the process into a finite-state machine with eventual periodicity.

### A.1 What the Discriminant Requirement Doesâ€”and Does Not Do

**Proposition A.1 (Meaning of Î” > 0)**

If Î” > 0, then the polynomial q(x) = axÂ² + bx + c has two distinct real roots, hence the parabola crosses the real axis twice.

*Proof.* Standard quadratic formula. âˆ

**What this doesn't imply:**

Î” > 0 does **not** by itself imply:

- Absence of fixed points or cycles on [0,1)
- Topological mixing or chaos for f(x) = {q(x)}
- An invariant distribution equal to uniform on [0,1)

In fact, fixed points can exist regardless of Î” (see A.2).

**Recommended interpretation:**

Î” > 0 is a convenient parameter-screening heuristic that prevents degenerate polynomial shapes and correlates empirically with better orbit behavior for our parameter families, but it is not a universal theorem guaranteeing chaos or uniformity.

### A.2 Fixed Points and Short Cycles Exist (In Principle)

**Proposition A.2 (Fixed Point Condition)**

A point x âˆˆ [0,1) is a fixed point of f(x) = {axÂ² + bx + c} iff there exists k âˆˆ â„¤ such that:

```
axÂ² + bx + c = x + k
âŸº axÂ² + (b-1)x + (c-k) = 0
```

*Proof.* f(x) = x means {axÂ² + bx + c} = x, i.e., axÂ² + bx + c = x + k for some integer k. Rearrange. âˆ

**Corollary A.3 (How Many Fixed Points Can Exist)**

For each integer k, the quadratic equation above has at most two real roots; therefore the map can have at most finitely many fixed points in [0,1), because only finitely many integers k can satisfy the range constraint:

```
k âˆˆ [min_{xâˆˆ[0,1]} (axÂ² + bx + c - x), max_{xâˆˆ[0,1]} (axÂ² + bx + c - x)]
```

**Practical meaning:** You cannot claim "constraints a > 0, c < 0, Î” > 0 rule out fixed points." They don't.

**Stronger, defensible implementation claim:**

Even though fixed points may exist, for a continuously chosen seed x_0, the probability of landing exactly on a fixed point is zero (it's a finite set). However:

- Under finite precision, "landing exactly" becomes possible, and short cycles are common in finite-state machines
- Therefore, reseed/mixing is not optional if you want "never get stuck" behavior

### A.3 Finite Precision Implies Eventual Periodicity (Hard Theorem)

Let the implementation store x_n with p bits of precision and round each update into that representation.

**Proposition A.4 (Eventual Periodicity Under Finite Precision)**

If the internal state is stored in a finite set ğ’® (e.g., all representable p-bit values in [0,1)), and the update is deterministic x_{n+1} = F(x_n), then the sequence is eventually periodic, with preperiod length < |ğ’®| and period length â‰¤ |ğ’®|.

*Proof.* Pigeonhole principle on the sequence of states. âˆ

**Engineering consequence:**

The claim "period is astronomically long / never repeats" is only defensible as:

> "With typical p (â‰¥ 53 bits) and our parameter families, empirically observed periods are extremely long relative to practical sample sizes; reseeding + cryptographic mixing makes the effective cycle length cryptographically large."

### A.4 When Do We Get Provable Statistical Mixing / Invariant Densities?

**Definition A.5 (Piecewise Expanding Interval/Circle Map)**

A map f : [0,1] â†’ [0,1] is piecewise CÂ² and uniformly expanding if there exists a finite partition into intervals on which f is CÂ², and:

```
inf |f'(x)| â‰¥ Î» > 1    (away from finitely many cut points)
```

**Theorem A.6 (Lasotaâ€“Yorke, Existence of ACIM)**

For a piecewise CÂ², uniformly expanding map of the interval, there exists an absolutely continuous invariant measure (ACIM); under mild extra conditions it is unique and the system is mixing with exponential decay of correlations.

**How it applies here (carefully):**

The map x â†¦ {axÂ² + bx + c} is piecewise smooth on [0,1), but it has a critical point where:

```
f'(x) = 2ax + b = 0    at    x* = -b/(2a)
```

So it is **not** uniformly expanding everywhere.

**Therefore:** You cannot claim a blanket theorem "this map is chaotic/uniform."

**What you can claim:**

> For parameter families where the induced mod-1 map behaves as a piecewise expanding unimodal map away from a negligible neighborhood of the critical point, one expects an ACIM and rapid mixing; our discriminant screening and empirical tests are designed to identify those parameters.

### A.5 Cryptographic Modular-Addition Mixing: Uniformity Is Provable

**Theorem A.7 (Haar Invariance on the Circle)**

Let U be any ğ•‹-valued random variable, and let C ~ Unif(ğ•‹) be independent. Then:

```
V := (U + C) mod 1 ~ Unif(ğ•‹)
```

*Proof.* Uniform on ğ•‹ is Haar measure. Convolution with Haar returns Haar: for any measurable A âŠ‚ ğ•‹,

```
â„™(V âˆˆ A) = âˆ«_ğ•‹ â„™(U âˆˆ A - c) dc = âˆ«_ğ•‹ Î¼(A) dc = Î¼(A)
```

âˆ

**Very strong corollary (important for security posture):**

If your "crypto_uniform" is truly uniform and independent (e.g., ChaCha20 from libsodium), then the final output is uniform regardless of the statistical imperfections of U.

This is the cleanest "hard guarantee" in the entire system.

### A.6 XOR Mixing: If One Operand Is Uniform, Output Is Uniform (Hard Theorem)

If you XOR mantissas, you can give a short discrete proof.

**Theorem A.8 (XOR with a Uniform Mask)**

Let A âˆˆ {0,1}^w be any bitstring-valued random variable, and let B ~ Unif({0,1}^w) be independent. Then:

```
A âŠ• B ~ Unif({0,1}^w)
```

*Proof.* For any y âˆˆ {0,1}^w,

```
â„™(A âŠ• B = y) = Î£_x â„™(A = x)â„™(B = x âŠ• y) = Î£_x â„™(A = x) Â· 2^{-w} = 2^{-w}
```

âˆ

**Takeaway:**

If any one of your combined streams is cryptographically uniform (or very close), XOR mixing makes the result uniform at the mantissa level.

### A.7 Averaging Does Not Preserve Uniformity (Hard Correction)

**Proposition A.9 (Mean of Two Uniforms Is Triangular)**

If U, C ~ iid Unif[0,1), then W = (U + C)/2 has density:

```
f_W(x) = { 4x,      0 â‰¤ x â‰¤ 1/2
         { 4(1-x),  1/2 < x â‰¤ 1
```

So "averaging" is a distribution transform, not a uniform-preserving mixer.

### A.8 Multi-Stream Modular Addition Reduces Bias via Fourier Damping

When combining multiple independent streams with addition mod 1, you can quantify "correlation breaking."

Let X, Y âˆˆ ğ•‹ be independent. Let Ï†_X(k) := ğ”¼[e^{2Ï€ikX}] be the k-th Fourier coefficient.

**Proposition A.10 (Convolution Multiplies Fourier Coefficients)**

If Z = (X + Y) mod 1, then:

```
Ï†_Z(k) = Ï†_X(k) Â· Ï†_Y(k)    âˆ€k âˆˆ â„¤
```

*Proof.*

```
Ï†_Z(k) = ğ”¼[e^{2Ï€ik(X+Y)}] = ğ”¼[e^{2Ï€ikX}]ğ”¼[e^{2Ï€ikY}]
```

by independence. âˆ

**Interpretation:**

Non-uniformity shows up as nonzero Ï†_X(k) for k â‰  0. Repeated modular-add mixing drives these toward zero multiplicatively, i.e., bias decays geometrically if the streams have |Ï†(k)| < 1.

This gives a real "math story" behind entropy combining beyond vibes.

### A.9 The 3Ã—3 Matrix Jump-Ahead Is Not Correct as Stated (Must Harden)

The identity:

```
[x_{n+1}]   [a b c] [x_nÂ²]
[x_n    ] = [1 0 0] [x_n ]
[1      ]   [0 0 1] [1   ]
```

is true for one step as an evaluation trick, but it does **not** imply that M^n performs an n-step jump.

**Proposition A.11 (Why M^n Does Not Encode Iteration)**

Let v_n := (x_nÂ², x_n, 1)^T. Even if you write x_{n+1} = [a b c]v_n, there is no fixed 3Ã—3 matrix M such that v_{n+1} = MÂ·v_n for all x_n, because:

```
x_{n+1}Â² = (ax_nÂ² + bx_n + c)Â²
```

is a degree-4 polynomial in x_n, hence cannot be expressed as a linear combination of (x_nÂ², x_n, 1) with constant coefficients.

*Proof.* If v_{n+1} = MÂ·v_n held with constant M, the first component of v_{n+1} would be linear in x_nÂ², x_n, 1â€”i.e., at most quadratic in x_n. But x_{n+1}Â² is quartic in x_n unless a = 0 (degenerate). Contradiction. âˆ

**Correct alternatives for jump-ahead:**

- **Stream splitting / leapfrogging**: Thread i uses x_{i+mt} for stride m (requires sequential generation but guarantees deterministic partitioning)
- **Parameter splitting**: Independent discriminants/parameter triplets per stream (our current approach)
- **Crypto counter-mode**: Generate C_n from ChaCha20(counter=n), then output (x_n + C_n) mod 1. This gives O(1) random access in n for the crypto component and preserves uniformity by Theorem A.7.

### A.10 Continued Fraction Periodicity: Proof Reference

**Lagrange's Theorem**: Quadratic irrationals have eventually periodic simple continued fraction expansions.

A slightly sharper "finite-state" argument for the Gaussâ€“Legendre algorithm:

**Proposition A.12 (Finite State Space Implies Repetition)**

In the Gaussâ€“Legendre recursion:

```
P_{n+1} = a_n Q_n - P_n
Q_{n+1} = (D - P_{n+1}Â²) / Q_n
```

one can show 0 â‰¤ P_n < âˆšD and 0 < Q_n â‰¤ âˆšD for reduced forms; hence there are only finitely many admissible integer pairs (P_n, Q_n), so some pair repeats and the continued fraction becomes periodic.

### A.11 "Military-grade" and "Cryptographic" Claims: How to Harden Safely

Granville's original write-up popularized the idea and includes empirical comparisons. But cryptographic security is a much stronger claim than "passes statistical tests."

**A safe, mathematically honest statement:**

> Without cryptographic mixing, this generator should be treated as a high-quality statistical PRNG (chaotic map on ğ•‹ with empirically screened parameters). With ChaCha20-based independent uniform mixing, the final output is provably uniform on [0,1) (Theorem A.7), and unpredictability reduces to the security of the underlying cryptographic stream.

This avoids overclaiming "QI core is cryptographically secure" (which would require actual cryptanalysis assumptions).

---

## Summary: Math Guarantees

### Guarantees (Provable)

1. **Finite precision implies eventual periodicity** (Proposition A.4)
2. **Modular-add crypto mixing yields exact uniformity on [0,1)** if crypto stream is uniform + independent (Theorem A.7)
3. **XOR mixing yields exact uniform mantissas** if any one operand is uniform + independent (Theorem A.8)
4. **CFE periodicity for quadratic irrationals** (Lagrange's theorem; A.10)
5. **Fourier damping**: Multi-stream modular addition reduces bias geometrically (Proposition A.10)

### Non-Guarantees (Must Be Empirical / Parameter-Dependent)

1. **Î” > 0 does not imply chaos, uniformity, or absence of fixed points** (A.1â€“A.2)
2. **The 3Ã—3 matrix form does not provide true O(log n) jump-ahead** (A.9)
3. **"Chaotic-looking" does not equal "cryptographically secure"** without crypto mixing (A.11)
4. **Fixed points may exist** for specific parameter combinations (A.2)
