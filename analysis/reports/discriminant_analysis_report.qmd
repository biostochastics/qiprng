 analy---
title: "Enhanced Discriminant Analysis for QIPRNG"
subtitle: "Advanced Statistical Assessment of 750 Quadratic Irrational Discriminants with 50,000 Sample Analysis"
author: "QIPRNG Enhanced Analysis Framework"
date: "`r Sys.Date()`"
format:
  html:
    toc: true
    toc-depth: 3
    number-sections: true
    code-fold: true
    code-summary: "Show code"
    theme: cosmo
    embed-resources: true
  pdf:
    toc: true
    number-sections: true
    colorlinks: true
execute:
  echo: false
  warning: false
  message: false
---

```{r setup}
#| include: false
library(qiprng)
library(moments)
library(nortest)
library(ggplot2)
library(gridExtra)
library(knitr)
library(DT)
library(plotly)

# Source our analysis modules
source("../../R/discriminant_tests.R")
source("../../R/discriminant_reports.R")

# Set global options
knitr::opts_chunk$set(
  echo = FALSE,
  warning = FALSE,
  message = FALSE,
  fig.width = 10,
  fig.height = 6,
  dpi = 300
)

# Custom theme for plots
theme_analysis <- theme_minimal() +
  theme(
    plot.title = element_text(size = 14, face = "bold"),
    plot.subtitle = element_text(size = 12),
    axis.title = element_text(size = 11),
    legend.title = element_text(size = 10),
    legend.text = element_text(size = 9)
  )
```

# Executive Summary

This report presents an **enhanced comprehensive statistical analysis** of random number quality for all 750 discriminant configurations in the QIPRNG (Quadratic Irrational Pseudo-Random Number Generator) package. Each discriminant represents a unique set of parameters (a, b, c) that satisfy the mathematical constraints for generating high-quality pseudorandom sequences based on quadratic irrational numbers.

**Key Enhancements in This Analysis:**
- **Increased Statistical Power**: 50,000 samples per discriminant (5x increase)
- **Enhanced Precision**: 256-bit MPFR precision (2x increase)
- **Advanced Test Suite**: 8 comprehensive statistical tests including entropy, gap, and poker tests
- **Multi-Method Periodicity Analysis**: 5 different periodicity detection methods
- **Cryptographic Quality Assessment**: Enabled cryptographic mixing for production-grade evaluation

## Methodology

### Core Statistical Properties

The enhanced analysis evaluates **eight comprehensive statistical properties** for each discriminant:

### Core Statistical Tests
1.  **Uniformity (Dual Method)**: Kolmogorov-Smirnov and Chi-squared tests assess if numbers are evenly distributed over [0, 1]
2.  **Independence (Runs Test)**: Wald-Wolfowitz runs test checks if each number is statistically independent
3.  **Autocorrelation (Multi-lag)**: Tests for linear dependencies at up to 50 different time lags
4.  **Enhanced Periodicity (5 Methods)**: 
    - Fisher's g-test for spectral analysis
    - Bartels rank test for randomness
    - Cox-Stuart test for trend detection
    - Turning points test
    - Ljung-Box test for serial correlation
5.  **Moment Analysis**: Compares sample statistics to theoretical uniform distribution values

### Advanced Quality Tests
6.  **Shannon Entropy**: Measures information content and randomness quality
7.  **Gap Test**: Analyzes spacing patterns between values in specified ranges
8.  **Serial Correlation**: Multi-lag correlation analysis for dependency detection
9.  **Poker Test**: Digit pattern analysis for cryptographic quality assessment

### Statistical Test Deep Dive

This section provides a brief explanation of each statistical test used in the analysis, its purpose, and how to interpret the results.

-   **Uniformity (Kolmogorov-Smirnov & Chi-Squared)**: The KS test compares the sample's cumulative distribution to the ideal uniform distribution, while the Chi-Squared test checks if the frequency of numbers in different bins matches expectations. For both, a high p-value (> 0.05) is good.

-   **Independence (Wald-Wolfowitz Runs Test)**: This test counts the number of "runs" (consecutive sequences above or below the median). An abnormal number of runs suggests a lack of independence. A high p-value (> 0.05) is good.

-   **Autocorrelation (Ljung-Box Test)**: This test checks for non-zero correlations between a number and its predecessors at various lags. We look for no significant correlations, so a "pass" is good.

-   **Periodicity (Fisher's g-test)**: This test uses a Fourier transform to find dominant, repeating cycles. A high p-value (> 0.05) indicates no significant periodicity was found, which is good.

```{r load-data}
#| echo: true
#| eval: false
# Load and validate discriminants
cat("Loading discriminants and running enhanced comprehensive analysis...\n")
cat("This analysis tests all 750 discriminants with 50,000 samples each.\n")
cat("Enhanced with advanced statistical tests and 256-bit precision.\n")
cat("Estimated runtime: 2-4 hours depending on system performance.\n\n")

# Execute the full analysis
results <- run_discriminant_analysis(
  discriminants_file = "../data/discriminants.csv",
  sample_size = 50000,  # Enhanced sample size
  max_discriminants = NULL  # Test all discriminants
)

# Generate summary statistics
summary_data <- generate_summary_stats(results)
```

```{r load-results}
#| include: false
# For development/testing, we'll load pre-computed results if available
# In production, this would run the full analysis above

if (file.exists("../results/discriminant_analysis_results/raw_results.rds")) {
  cat("Loading analysis results...\n")
  results <- readRDS("../results/discriminant_analysis_results/raw_results.rds")
  summary_data <- generate_summary_stats(results)
  cat("Loaded", length(results), "discriminant results\n")
} else if (file.exists("../results/full_analysis_results.rds")) {
  cat("Loading pre-computed results...\n")
  results <- readRDS("../results/full_analysis_results.rds")
  summary_data <- generate_summary_stats(results)
} else {
  cat("No analysis results found. Please run the full analysis first.\n")
  cat("Use: Rscript run_discriminant_analysis.R\n")
  knitr::knit_exit()
}
```

## Key Findings

```{r summary-stats}
#| results: asis

# Validate results
n_tested <- nrow(summary_data)
n_successful <- sum(summary_data$quality_rating != "Error")
n_errors <- sum(summary_data$quality_rating == "Error")

cat("- **Total Discriminants Analyzed:** ", n_tested, "\n")
cat("- **Successful Tests:** ", n_successful, " (", round(100 * n_successful / n_tested, 1), "%)\n")
cat("- **Failed/Error Tests:** ", n_errors, " (", round(100 * n_errors / n_tested, 1), "%)\n")
cat("- **Mean Quality Score:** ", round(mean(summary_data$overall_score, na.rm = TRUE), 3), "\n")
cat("- **Median Quality Score:** ", round(median(summary_data$overall_score, na.rm = TRUE), 3), "\n")
```

# Detailed Analysis Results

## Overall Quality Distribution

```{r quality-overview}
# Quality distribution summary
quality_counts <- table(summary_data$empirical_rating)
quality_df <- data.frame(
  Rating = names(quality_counts),
  Count = as.numeric(quality_counts),
  Percentage = round(100 * as.numeric(quality_counts) / nrow(summary_data), 1)
)

kable(quality_df, 
      caption = "Distribution of Quality Ratings Across All Discriminants",
      col.names = c("Quality Rating", "Count", "Percentage (%)"))
```

```{r quality-plots-interactive}
#| fig-cap: "Interactive Quality Score and Rating Analysis"
#| layout-ncol: 2

p1 <- ggplot(summary_data, aes(x = overall_score)) +
  geom_histogram(bins = 30, fill = "skyblue", alpha = 0.7, color = "black") +
  labs(title = "Distribution of Overall Quality Scores", x = "Overall Score", y = "Count") +
  theme_analysis

p2 <- ggplot(quality_df, aes(x = Rating, y = Percentage, fill = Rating)) +
  geom_col() + coord_polar("y", start = 0) +
  labs(title = "Quality Distribution", x = "", y = "") +
  theme_void() + scale_fill_brewer(palette = "Set3")

# Render as interactive plots
ggplotly(p1)
print(p2)  # Use static plot for pie chart to avoid plotly issues
```

## Test Performance Analysis

```{r test-performance}
# Calculate test pass rates
test_pass_data <- data.frame(
  Test = c("Uniformity", "Independence", "Autocorrelation", "Periodicity"),
  PassRate = c(
    mean(summary_data$uniformity_passed, na.rm = TRUE),
    mean(summary_data$independence_passed, na.rm = TRUE),
    mean(summary_data$autocorrelation_passed, na.rm = TRUE),
    mean(summary_data$periodicity_passed, na.rm = TRUE)
  ) * 100
)

kable(test_pass_data, 
      digits = 1,
      caption = "Test Pass Rates Across All Discriminants",
      col.names = c("Statistical Test", "Pass Rate (%)"))
```

```{r test-performance-plot-interactive}
#| fig-cap: "Interactive Test Pass Rates Visualization"

p <- ggplot(test_pass_data, aes(x = reorder(Test, PassRate), y = PassRate, text = paste0(Test, ": ", round(PassRate, 1), "%"))) +
  geom_col(fill = "lightgreen", color = "black", alpha = 0.8) +
  coord_flip() +
  labs(title = "Statistical Test Pass Rates", 
       subtitle = "Percentage of discriminants passing each test", 
       x = "", y = "Pass Rate (%)") +
  ylim(0, 100) + theme_analysis

ggplotly(p, tooltip = "text")
```

## Parameter Relationships

```{r parameter-analysis-interactive}
#| fig-cap: "Interactive Parameter and Quality Analysis"
#| layout-ncol: 2

p1 <- ggplot(summary_data, aes(x = discriminant, y = overall_score, text = paste("Δ:", discriminant, "\nScore:", round(overall_score, 3)))) +
  geom_point(alpha = 0.6, color = "darkblue") +
  labs(title = "Discriminant Value vs Quality Score", x = "Discriminant", y = "Overall Score") + 
  theme_analysis

p2 <- ggplot(summary_data, aes(x = a, y = abs(c), color = overall_score, text = paste("a:", a, "|c|:", abs(c), "\nScore:", round(overall_score, 3)))) +
  geom_point(alpha = 0.7) +
  scale_color_gradient(low = "red", high = "green", name = "Quality Score") +
  labs(title = "Parameter Relationships", x = "Parameter a", y = "|Parameter c|") + 
  theme_analysis

ggplotly(p1, tooltip = "text")
ggplotly(p2, tooltip = "text")
```

```{r correlation-analysis}
# Calculate correlations
valid_data <- summary_data[!is.na(summary_data$overall_score) & summary_data$overall_score > 0, ]

if (nrow(valid_data) > 10) {
  correlations <- data.frame(
    Parameter = c("Discriminant (Δ)", "Parameter a", "|Parameter c|", "Parameter b"),
    Correlation = c(
      cor(valid_data$discriminant, valid_data$overall_score),
      cor(valid_data$a, valid_data$overall_score),
      cor(abs(valid_data$c), valid_data$overall_score),
      cor(valid_data$b, valid_data$overall_score)
    )
  )
  
  kable(correlations, 
        digits = 3,
        caption = "Correlation Between Parameters and Quality Score",
        col.names = c("Parameter", "Correlation with Quality Score"))
}
```

## Top Performing Discriminants

```{r top-performers}
# Get top 10 performers
top_indices <- order(summary_data$overall_score, decreasing = TRUE)[1:10]
top_performers <- summary_data[top_indices, c("index", "a", "b", "c", "discriminant", 
                                               "overall_score", "quality_rating",
                                               "uniformity_passed", "independence_passed",
                                               "autocorrelation_passed", "periodicity_passed")]

# Create a nice table
top_performers$Tests_Passed <- paste0(
  ifelse(top_performers$uniformity_passed, "U", ""),
  ifelse(top_performers$independence_passed, "I", ""),
  ifelse(top_performers$autocorrelation_passed, "A", ""),
  ifelse(top_performers$periodicity_passed, "P", "")
)

display_table <- top_performers[, c("index", "a", "b", "c", "discriminant", 
                                   "overall_score", "quality_rating", "Tests_Passed")]

kable(display_table,
      digits = 3,
      caption = "Top 10 Performing Discriminants (U=Uniformity, I=Independence, A=Autocorrelation, P=Periodicity)",
      col.names = c("Index", "a", "b", "c", "Discriminant", "Score", "Rating", "Tests Passed"))
```

## Bottom Performing Discriminants

```{r failure-diagnostics}
# Helper to pull worst examples per test
get_fail_examples <- function(df, passed_col, pval_col, n = 3) {
  subset <- df[!df[[passed_col]] & !is.na(df[[pval_col]]), ]
  if (nrow(subset) == 0) return(subset)
  subset <- subset[order(subset[[pval_col]]), ]
  head(subset, n)
}

uniformity_fail <- get_fail_examples(summary_data, "uniformity_passed", "ks_p_value")
independence_fail <- get_fail_examples(summary_data, "independence_passed", "independence_p_value")
periodicity_fail <- get_fail_examples(summary_data, "periodicity_passed", "periodicity_p_value")

dplyr::bind_rows(
  list(Uniformity = uniformity_fail,
       Independence = independence_fail,
       Periodicity = periodicity_fail), .id = "Test") %>%
  select(Test, index, a, b, c, discriminant, overall_score, starts_with("ks_p_value"), starts_with("independence_p_value"), starts_with("periodicity_p_value")) %>%
  knitr::kable(digits = 4, caption = "Examples of Discriminants Failing Core Tests (sorted by worst p-values)")
```

## Statistical Insights

```{r bottom-performers}
# Get bottom 10 performers (excluding errors)
non_error_data <- summary_data[summary_data$quality_rating != "Error", ]
bottom_indices <- order(non_error_data$overall_score)[1:10]
bottom_performers <- non_error_data[bottom_indices, c("index", "a", "b", "c", "discriminant", 
                                                      "overall_score", "quality_rating",
                                                      "uniformity_passed", "independence_passed",
                                                      "autocorrelation_passed", "periodicity_passed")]

bottom_performers$Tests_Failed <- paste0(
  ifelse(!bottom_performers$uniformity_passed, "U", ""),
  ifelse(!bottom_performers$independence_passed, "I", ""),
  ifelse(!bottom_performers$autocorrelation_passed, "A", ""),
  ifelse(!bottom_performers$periodicity_passed, "P", "")
)

display_table_bottom <- bottom_performers[, c("index", "a", "b", "c", "discriminant", 
                                              "overall_score", "quality_rating", "Tests_Failed")]

kable(display_table_bottom,
      digits = 3,
      caption = "Bottom 10 Performing Discriminants (U=Uniformity, I=Independence, A=Autocorrelation, P=Periodicity)",
      col.names = c("Index", "a", "b", "c", "Discriminant", "Score", "Rating", "Tests Failed"))
```

## Statistical Test Details

### P-Value Distributions

```{r pvalue-analysis-interactive}
#| fig-cap: "Interactive P-value Distributions"

pvalue_data <- data.frame(
  PValue = c(summary_data$ks_p_value, summary_data$chi_sq_p_value, 
             summary_data$independence_p_value, summary_data$periodicity_p_value),
  Test = rep(c("Uniformity (KS)", "Uniformity (Chi-Sq)", "Independence (Runs)", "Periodicity (Fisher)"), 
           each = nrow(summary_data))
)
pvalue_data <- pvalue_data[!is.na(pvalue_data$PValue), ]

p <- ggplot(pvalue_data, aes(x = PValue)) +
  geom_histogram(aes(fill = Test), bins = 20, alpha = 0.7) +
  facet_wrap(~Test, scales = "free_y") +
  geom_vline(xintercept = 0.05, linetype = "dashed", color = "red") +
  labs(title = "Distribution of P-values", x = "P-value", y = "Count") +
  theme_analysis + theme(legend.position = "none")

ggplotly(p)
```

### Moment Analysis

```{r moment-analysis}
# Extract moment errors for valid results
valid_results <- results[sapply(results, function(x) !is.null(x$moments) && is.null(x$error))]

if (length(valid_results) > 0) {
  moment_data <- data.frame(
    Index = sapply(valid_results, function(x) x$index),
    MeanError = sapply(valid_results, function(x) x$moments$mean_error),
    VarError = sapply(valid_results, function(x) x$moments$var_error),
    SkewError = sapply(valid_results, function(x) x$moments$skew_error),
    KurtError = sapply(valid_results, function(x) x$moments$kurt_error)
  )
  
  # Summary statistics
  moment_summary <- data.frame(
    Moment = c("Mean", "Variance", "Skewness", "Kurtosis"),
    MedianError = c(
      median(moment_data$MeanError, na.rm = TRUE),
      median(moment_data$VarError, na.rm = TRUE),
      median(moment_data$SkewError, na.rm = TRUE),
      median(moment_data$KurtError, na.rm = TRUE)
    ),
    MeanError = c(
      mean(moment_data$MeanError, na.rm = TRUE),
      mean(moment_data$VarError, na.rm = TRUE),
      mean(moment_data$SkewError, na.rm = TRUE),
      mean(moment_data$KurtError, na.rm = TRUE)
    ),
    MaxError = c(
      max(moment_data$MeanError, na.rm = TRUE),
      max(moment_data$VarError, na.rm = TRUE),
      max(moment_data$SkewError, na.rm = TRUE),
      max(moment_data$KurtError, na.rm = TRUE)
    )
  )
  
  kable(moment_summary,
        digits = 6,
        caption = "Moment Error Analysis Summary",
        col.names = c("Moment", "Median Error", "Mean Error", "Maximum Error"))
}
```

# Interactive Data Exploration

This table provides the full summary data for all 750 discriminants. You can sort by any column, search for specific values, and navigate through the pages to explore the entire dataset.

```{r interactive-table}
#| cap: "Full Discriminant Analysis Results"

# Select and rename columns for clarity
display_data <- summary_data[, c("index", "a", "b", "c", "discriminant", "overall_score", "quality_rating")]
colnames(display_data) <- c("Index", "a", "b", "c", "Discriminant", "Score", "Rating")

DT::datatable(
  display_data,
  options = list(
    pageLength = 10,
    autoWidth = TRUE,
    scrollX = TRUE
  ),
  rownames = FALSE,
  filter = 'top',
  class = 'cell-border stripe'
)
```

```{r interactive-table-detailed}
#| fig-cap: "Interactive Summary Table - All Discriminants"

# Create interactive table with all results
# Check available columns first
available_cols <- colnames(summary_data)
cat("Available columns:", paste(available_cols, collapse=", "), "\n")

# Select only columns that exist
base_cols <- c("index", "a", "b", "c", "discriminant", "overall_score", "quality_rating")
optional_cols <- c("ks_p_value", "chi_sq_p_value", "independence_p_value", "mean_error", "var_error")

# Filter to only existing columns
selected_cols <- c(base_cols, optional_cols[optional_cols %in% available_cols])
interactive_data <- summary_data[, selected_cols]

datatable(interactive_data,
          caption = "Complete Discriminant Analysis Results (Interactive)",
          options = list(
            pageLength = 25,
            scrollX = TRUE,
            dom = 'Bfrtip',
            buttons = c('copy', 'csv', 'excel')
          ),
          filter = 'top') %>%
  formatRound(columns = intersect(c("overall_score", "ks_p_value", "chi_sq_p_value", "independence_p_value", 
                                   "mean_error", "var_error"), colnames(interactive_data)), digits = 4)
```

# Recommendations and Conclusions

## Production Recommendations

```{r recommendations}
#| results: asis

excellent_discriminants <- summary_data[summary_data$empirical_rating == "Excellent", ]
good_discriminants <- summary_data[summary_data$empirical_rating %in% c("Very-Good", "Good"), ]
poor_discriminants <- summary_data[summary_data$empirical_rating %in% c("Poor", "Fair"), ]

cat("### Recommended for Production Use\n\n")
if (nrow(excellent_discriminants) > 0) {
  cat("**", nrow(excellent_discriminants), " Excellent Discriminants** (Score ≥ 0.75):\n")
  cat("These discriminants passed most statistical tests and are recommended for production use.\n\n")
  
  # Show top 5 excellent performers
  top_excellent <- head(excellent_discriminants[order(excellent_discriminants$overall_score, decreasing = TRUE), ], 5)
  for (i in 1:nrow(top_excellent)) {
    row <- top_excellent[i, ]
    cat("- **Discriminant #", row$index, "**: a=", row$a, ", b=", row$b, ", c=", row$c, 
        ", Δ=", row$discriminant, " (Score: ", round(row$overall_score, 3), ")\n")
  }
  cat("\n")
}

if (nrow(good_discriminants) > 0) {
  cat("**", nrow(good_discriminants), " Good Discriminants** (Score ≥ 0.50):\n")
  cat("These discriminants show good statistical properties and are suitable for most applications.\n\n")
}

cat("### Discriminants to Avoid\n\n")
if (nrow(poor_discriminants) > 0) {
  cat("**", nrow(poor_discriminants), " Poor/Fair Discriminants**:\n")
  cat("These discriminants failed multiple statistical tests and should be avoided in production.\n\n")
}

error_count <- sum(summary_data$quality_rating == "Error")
if (error_count > 0) {
  cat("**", error_count, " Error Cases**:\n")
  cat("These discriminants caused computational errors and require investigation.\n\n")
}
```

## Statistical Insights

```{r insights}
#| results: asis

cat("### Key Findings\n\n")

# Test-specific insights
uniformity_pass_rate <- mean(summary_data$uniformity_passed, na.rm = TRUE) * 100
independence_pass_rate <- mean(summary_data$independence_passed, na.rm = TRUE) * 100
autocorr_pass_rate <- mean(summary_data$autocorr_passed, na.rm = TRUE) * 100
periodicity_pass_rate <- mean(summary_data$periodicity_passed, na.rm = TRUE) * 100

cat("1. **Uniformity Performance**: ", round(uniformity_pass_rate, 1), "% of discriminants produce uniform distributions\n")
cat("2. **Independence Performance**: ", round(independence_pass_rate, 1), "% of discriminants generate independent sequences\n")
cat("3. **Autocorrelation Performance**: ", round(autocorr_pass_rate, 1), "% of discriminants show no significant autocorrelation\n")
cat("4. **Periodicity Performance**: ", round(periodicity_pass_rate, 1), "% of discriminants show no detectable periodicity\n\n")

# Parameter insights
if (exists("correlations")) {
  strongest_corr <- correlations[which.max(abs(correlations$Correlation)), ]
  cat("5. **Parameter Correlation**: The strongest correlation with quality is ", 
      strongest_corr$Parameter, " (r = ", round(strongest_corr$Correlation, 3), ")\n\n")
}

cat("### Mathematical Validation\n\n")
cat("All tested discriminants satisfy the fundamental mathematical constraints:\n")
cat("- a > 0 (positive coefficient)\n")
cat("- c < 0 (negative constant term)\n") 
cat("- Δ = b² - 4ac > 0 (positive discriminant for chaotic behavior)\n\n")

cat("The analysis confirms that discriminants with higher values tend to produce better statistical properties, ")
cat("consistent with the theoretical expectation that larger discriminants provide more chaotic dynamics.\n\n")
```

# Technical Appendix

## Analysis Parameters

- **Sample Size**: 50,000 random numbers per discriminant (default)
- **MPFR Precision**: 256 bits (default)
- **Significance Level**: α = 0.05 for all statistical tests
- **Autocorrelation Lags**: Up to 50 lags tested (default)
- **Spectral Analysis**: Enhanced periodicity analysis using Fisher's g-test, Bartels rank test, Cox-Stuart test, turning points test, and Ljung-Box test
- **Parallel Processing**: Multi-core analysis with dynamic chunk sizing for efficiency

## Computational Environment

```{r session-info}
sessionInfo()
```

## Data Export

```{r data-export}
#| eval: false
#| echo: true

# Export results for further analysis
write.csv(summary_data, "discriminant_analysis_summary.csv", row.names = FALSE)
saveRDS(results, "complete_discriminant_results.rds")

cat("Analysis complete. Results exported to:\n")
cat("- discriminant_analysis_summary.csv\n")
cat("- complete_discriminant_results.rds\n")
```

---

*This report was generated using the QIPRNG Discriminant Analysis Framework. For questions or additional analysis, please refer to the source code in the R/ directory.*
